<?xml version="1.0" encoding="utf-8"?>
<ArrayOfProgramBlock xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
  <ProgramBlock>
    <ScriptSetup>Program.Run();
</ScriptSetup>
    <ScriptSource>// ===================================================================
// CONTEXT ENGINE - SYSTEM UTILITY
// ===================================================================
// This service generates dynamic prompts for LLM handlers.
// It populates templates with the list of controllable devices and
// current system status (Weather, Energy, Security, and live states).
// ===================================================================

Api.Handle("System.Utility/ContextEngine", (args) =&gt; {
    var request = Api.Parse(args);
    string template = (string)request.Data;

    if (string.IsNullOrEmpty(template))
    {
        template = "**AVAILABLE MODULES**\n\n%%AVAILABLE_DEVICES%%\n**CURRENT SYSTEM STATUS** (Timestamp: %%CURRENT_TIME%%)\n\n%%SYSTEM_STATUS%%\n";
    }

    // --- 1. COLLECT USER-CONFIGURED MODULES ---
    // We create a unified list of ModuleHelpers that the user is allowed to control.
    var controlModules = new List&lt;ModuleHelper&gt;();
    var sensorModules = new List&lt;ModuleHelper&gt;();

    // Physical devices (Lights, Switches, Thermostats, etc.)
    string allowedTypes = "Dimmer,Light,Color,Switch,Shutter,DoorLock,Thermostat,Sensor,DoorWindow";
    Modules.OfDeviceType(allowedTypes).Each((m) =&gt; {
        var instance = m.Instance;
        var deviceType = instance.DeviceType.ToString().ToLower();
        if ((deviceType == "sensor" || deviceType == "doorwindow") &amp;&amp; instance.Domain != "HomeAutomation.SecuritySystem")
        {
            sensorModules.Add(m);
            // the only "Sensor" listed in 'controlModules' is Security Alarm System
        } else {
            controlModules.Add(m);
        }
        return false; // Continue iteration
    });

    // Scenarios and Macros (Programs configured with the generic program widget)
    Modules.OfDeviceType("Program").WithParameter("Widget.DisplayModule").Each((m) =&gt; {
        // Filter out system loggers and ensure it's a user-facing program
        var widget = m.Parameter("Widget.DisplayModule")?.Value;
        if (m.Instance.Address != "6" &amp;&amp; m.Instance.Address != "7"
            &amp;&amp; m.Instance.Address != "34" &amp;&amp; m.Instance.Address != "940"
            &amp;&amp; !String.IsNullOrWhiteSpace(widget))
        {
            controlModules.Add(m);
        }
        return false;
    });

    // --- 2. GENERATE AVAILABLE DEVICES LIST (Directory) ---
    // This section provides the AI with a clean list of IDs and types.
    // Live states (ON/OFF) are NOT included here to keep the list stable.
    var devicesSb = new StringBuilder();
    foreach (var m in controlModules) {
        var module = m.Instance;
        var type = module.DeviceType.ToString();
        // exception
        if (type == "Program" &amp;&amp; m.Parameter("Widget.DisplayModule").Value != "homegenie/generic/program")
        {
            type = m.Parameter("Widget.DisplayModule").Value;
            if (type.LastIndexOf("/") &gt; 0)
            {
                type = type.Substring(type.LastIndexOf("/") + 1);
            }
        }

        var moduleGroups = Modules.Groups
            .Where(g =&gt; Modules.InGroup(g).SelectedModules.Any(mod =&gt;
                mod.Domain == module.Domain &amp;&amp;
                mod.Address == module.Address))
            .ToList();

        var groups = string.Join(", ", moduleGroups);
        if (string.IsNullOrEmpty(groups))
        {
            groups = "(unassigned)";
        }

        devicesSb.AppendLine($"- {module.Name} | {type} | {module.Domain}/{module.Address} | " + groups);
    }

    // --- 3. GENERATE SYSTEM STATUS REPORT (Live Context) ---
    // This section provides the AI with real-time situational awareness.
    var statusSb = new StringBuilder();

    // A. Core System Reports (Weather, Energy, Alarm, Master Thermostat)
    statusSb.AppendLine("## SYSTEM REPORT");

    var weather = Modules.InDomain("HomeAutomation.HomeGenie.Automation").WithAddress("34").Get();
    if (weather.Instance != null) statusSb.AppendLine("- " + PromptHelpers.GetWeatherStatus(weather));

    var energy = Modules.InDomain("HomeAutomation.EnergyMonitor").WithAddress("1").Get();
    if (energy.Instance != null) statusSb.AppendLine("- " + PromptHelpers.GetEnergyStatus(energy));

    var alarm = Modules.InDomain("HomeAutomation.SecuritySystem").WithAddress("Main").Get();
    if (alarm.Instance != null) statusSb.AppendLine("- " + PromptHelpers.GetSecurityStatus(alarm));

    var thermo = Modules.InDomain("HomeAutomation.SmartThermostat").WithAddress("1").Get();
    if (thermo.Instance != null) statusSb.AppendLine("- " + PromptHelpers.GetThermostatStatus(thermo));

    // B. Individual Devices Live Status
    // Provides a 1:1 status update for the devices listed in the directory.
    statusSb.AppendLine("\n## DEVICES STATUS");
    foreach (var m in controlModules) {
        var info = PromptHelpers.GetModuleStatus(m);
        if (!string.IsNullOrEmpty(info)) statusSb.AppendLine("- " + info);
    }
    statusSb.AppendLine("\n## SENSORS STATUS");
    foreach (var m in sensorModules) {
        var info = PromptHelpers.GetModuleStatus(m);
        if (!string.IsNullOrEmpty(info)) statusSb.AppendLine("- " + info);
    }

    // --- 4. TEMPLATE ASSEMBLY ---
    // Replace placeholders with generated content and current server time.
    return template
        .Replace("%%AVAILABLE_DEVICES%%", devicesSb.ToString())
        .Replace("%%SYSTEM_STATUS%%", statusSb.ToString())
        .Replace("%%CURRENT_TIME%%", DateTime.Now.ToString("f", System.Globalization.CultureInfo.InvariantCulture)
        + "\n--- END OF SYSTEM REPORT ---\n\n");
});

// Run in background to stay available for LLM handler requests
Program.GoBackground();
</ScriptSource>
    <ScriptContext>public static class PromptHelpers
{
    /**
     * Helper to format double values for the AI.
     * Ensures the dot is always used as the decimal separator (Invariant Culture).
     */
    public static string Fmt(double val, string unit = "")
    {
        if (double.IsNaN(val) || double.IsInfinity(val)) return "N/A";
        return val.ToString("F1", CultureInfo.InvariantCulture) + unit;
    }

    /**
     * Extracts weather conditions, exterior temperature, and wind speed.
     */
    public static string GetWeatherStatus(ModuleHelper m)
    {
        var condition = m.Parameter("Conditions.Status").Value;
        if (string.IsNullOrWhiteSpace(condition)) condition = m.Parameter("Conditions.Description").Value;

        var temp = Fmt(m.Parameter("Sensor.Temperature").DecimalValue, "°C");
        var wind = Fmt(m.Parameter("Sensor.Wind.Speed").DecimalValue, " Kph");

        return $"Weather: {(string.IsNullOrEmpty(condition) ? "Unknown" : condition)} | Exterior Temp: {temp} | Wind: {wind}";
    }

    /**
     * Summarizes energy load, daily usage, and active device counts.
     */
    public static string GetEnergyStatus(ModuleHelper m)
    {
        var load = Fmt(m.Parameter("EnergyMonitor.WattLoad").DecimalValue, "W");
        var todayKwh = (m.Parameter("EnergyMonitor.WattCounter.Today").DecimalValue / 1000.0).ToString("F2", CultureInfo.InvariantCulture) + " kWh";

        var lights = m.Parameter("EnergyMonitor.OperatingLights").Value;
        var appliances = m.Parameter("EnergyMonitor.OperatingSwitches").Value;

        return $"Energy: Load {load} | Today's Usage: {todayKwh} (Active -&gt; Lights: {lights}, Appliances: {appliances})";
    }

    /**
     * Reports the alarm arming mode and whether the security system is currently triggered.
     */
    public static string GetSecurityStatus(ModuleHelper m)
    {
        var state = m.Parameter("HomeGenie.SecurityArmed").Value;
        var triggered = m.Parameter("HomeGenie.SecurityTriggered").DecimalValue &gt; 0 ? " !!ALARM TRIGGERED!!" : "";

        return $"Security: System is {state}{triggered}";
    }

    /**
     * Reports ambient temperature, target setpoint, and the current operating state.
     */
    public static string GetThermostatStatus(ModuleHelper m)
    {
        var ambient = Fmt(m.Parameter("Sensor.Temperature").DecimalValue, "°C");
        var target = Fmt(m.Parameter("Thermostat.SetPoint.Heating").DecimalValue, "°C");
        var state = m.Parameter("Thermostat.OperatingState").Value;
        var mode = m.Parameter("Thermostat.Mode").Value;

        return $"Thermostat: Ambient {ambient} | Target {target} (Mode: {mode}, State: {state})";
    }

    /**
     * Extracts a concise status string for any module (Sensors, Actuators, or Programs).
     * Used for the dynamic System Update/Status report.
     */
    public static string GetModuleStatus(ModuleHelper m)
    {
        // Skip modules with no name to keep the prompt clean
        if (string.IsNullOrWhiteSpace(m.Instance.Name)) return "";

        var statusParts = new List&lt;string&gt;();

        // 1. PROGRAMS (Scenarios/Macros)
        if (m.Instance.DeviceType.ToString() == "Program")
        {
            var pStatus = m.Parameter("Program.Status").Value;
            statusParts.Add("Status: " + (string.IsNullOrEmpty(pStatus) ? "Idle" : pStatus));
        }

        // 2. SENSORS (Environmental parameters)
        if (m.HasParameter("Sensor.Temperature"))
            statusParts.Add("Temp: " + Fmt(m.Parameter("Sensor.Temperature").DecimalValue, "°C"));

        if (m.HasParameter("Sensor.Humidity"))
            statusParts.Add("Hum: " + Fmt(m.Parameter("Sensor.Humidity").DecimalValue, "%"));

        if (m.HasParameter("Sensor.Luminance"))
            statusParts.Add("Luminance: " + m.Parameter("Sensor.Luminance").Value + " lux");

        if (m.HasParameter("Sensor.Motion"))
            statusParts.Add("Motion: " + (m.Parameter("Sensor.Motion").DecimalValue &gt; 0 ? "Detected" : "None"));

        if (m.HasParameter("Meter.Watts"))
            statusParts.Add("Watts: " + Fmt(m.Parameter("Meter.Watts").DecimalValue, "W"));

        if (m.HasParameter("Meter.Watts.Hour"))
            statusParts.Add("Watthour: " + Fmt(m.Parameter("Meter.Watts.Hour").DecimalValue, "W/h"));

        if (m.HasParameter("Sensor.DoorWindow"))
            statusParts.Add("Door/Window: " + (m.Parameter("Sensor.DoorWindow").DecimalValue &gt; 0 ? "Open" : "Closed"));

        // 3. ACTUATORS (Lights, Switches, Dimmer, Shutters)
        // Reports live state (ON/OFF/%) for the system status update
        if (m.HasParameter("Status.Level"))
        {
            var level = m.Parameter("Status.Level").DecimalValue;
            string state = (level &gt; 0) ? "ON" : "OFF";

            // Append percentage for dimmers or color lights
            if (level &gt; 0 &amp;&amp; level &lt;= 1) state = "ON"; // Standard binary
            else if (level &gt; 1) state += $" ({level}%)";

            statusParts.Add("State: " + state);
        }

        // Final fallback to ensure the AI knows the device is reachable
        if (statusParts.Count == 0) statusParts.Add("Status: Online");

        return $"{m.Instance.Name}: {string.Join(", ", statusParts)}";
    }
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>context-engine</Id>
      <Version>1.0.3</Version>
      <Required>true</Required>
      <Checksum>0E7FEC9D423A27CE2D47D79047C5CB1A</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>578</Address>
    <Name>Context Engine</Name>
    <Description>Centralized service for dynamic AI prompt generation. It populates templates with the list of controllable devices and the live system status (weather, energy, sensors), ensuring total real-time home awareness for the AI.</Description>
    <Group>Utilities</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>// Path to the external markdown file for system instructions
var systemPromptFile = Path.Combine(Data.GetFolder(), "gemini_system_prompt.txt");

// This method is used to register this program as a "AI intent handler"
Program.Implements(
    "@AI:IntentHandler",
    API_URL
).AddOption(OPTION_API_KEY, "", "1. Google Gemini AI API key", "text")
 .AddOption(OPTION_MODEL_ID, DEFAULT_MODEL_ID, "2. Model ID", "text")
 .AddOption(OPTION_SEND_CONTEXT_DATA, "", "3. Share system and module context with the AI", "checkbox")
 .AddOption(OPTION_SYSTEM_INSTRUCTIONS, "", "4. System Instructions", $"editor:file:{systemPromptFile}:markdown")
 .Run();
</ScriptSetup>
    <ScriptSource>// 1. Validate configuration
string geminiApiKey = Program.Option("ApiKey").Value.Trim();
string geminiModelId = Program.Option("ModelId").Value.Trim();
if (geminiApiKey.Length != 39 || !geminiApiKey.StartsWith("AIzaSy") || string.IsNullOrEmpty(geminiModelId))
{
    string error = "Gemini Intent Handler is not properly configured.";
    //Program.Notify($"ERROR: {error} Please configure the program options.");
    Program.Emit("Status.Init", "NotConfigured");
}
else
{
    Program.Emit("Status.Init", "Ready");
}

bool reconfigureRequired = false;
// Register event handler to trigger a restart if configuration options change.
When.ModuleParameterChanged((module, parameter) =&gt; {
  if (module.Instance == Program.Module &amp;&amp; parameter.Name.StartsWith("ConfigureOptions.")) {
    reconfigureRequired = true;
  }
  return true;
});

// 2. Build the current device list
string allowedTypes = "Dimmer,Light,Color,Switch,Shutter,DoorLock,Thermostat";

// ===================================================================
// API HANDLER: Submit a new intent (user command)
// ===================================================================
Api.Handle($"{API_URL}/{API_PROMPT_SUBMIT}", (userCommand) =&gt;
{
    // Get the current history from the centralized service
    var storeId = Program.Module.Address;
    var result = Api.Call("System.Utility/ChatHistory/Get", new { StoreId = storeId });
    var payloadContents = JArray.FromObject(result).ToObject&lt;List&lt;ChatMessage&gt;&gt;();

    // Prepare the current user message
    var userMessage = ChatMessage.Create("user", userCommand as string);

    // Combine them for the AI request payload
    // This list now contains [History] + [Current Message]
    payloadContents.Add(userMessage);

    // Path to the external markdown file for system instructions
    var systemPromptFile = Path.Combine(Data.GetFolder(), "gemini_system_prompt.txt");
    var promptText = "";
    try { promptText = File.ReadAllText(systemPromptFile); } catch (Exception e) { /* Handle error */ }

    bool sendContextData = (Program.Option(OPTION_SEND_CONTEXT_DATA).Value == "On");
    if (sendContextData)
    {
        promptText = promptText.Replace("\n%%AVAILABLE_DEVICES%%", "%%AVAILABLE_DEVICES%%\n\n%%SYSTEM_STATUS%%");
    }

    var systemPrompt = (string)Api.Call("System.Utility/ContextEngine/Render", promptText);
    var payload = new
    {
        contents = payloadContents,
        system_instruction = new { parts = new[] { new { text = systemPrompt } } },
        generationConfig = new { temperature = 0.3 }
    };

    try
    {
        // 4. Send the request to Gemini API
        string payloadJson = JsonConvert.SerializeObject(payload);
        string geminiRequestUrl = $"https://generativelanguage.googleapis.com/v1beta/models/{geminiModelId}:generateContent?key={geminiApiKey}";
        var data = Net.WebService(geminiRequestUrl).WithTimeout(120).AddHeader("Content-Type", "application/json").Post(payloadJson).GetData();

        // 5. Process the response
        string rawGeminiResponseJson = data?.candidates?[0]?.content?.parts?[0]?.text;

        if (string.IsNullOrEmpty(rawGeminiResponseJson))
        {
            string error = "Received an empty or invalid response from the AI.";
            Program.Notify($"ERROR: {error}");
            return new ResponseText($"**Error:** {error}");
        }

        string jsonResponse = rawGeminiResponseJson;
        string thoughtsChain = "";

        string patternJsonBlock = @"```json\s*({[\s\S]*?})\s*```";
        var matchJsonBlock = Regex.Match(rawGeminiResponseJson, patternJsonBlock, RegexOptions.Singleline);

        if (matchJsonBlock.Success)
        {
            jsonResponse = matchJsonBlock.Groups[1].Value;
            thoughtsChain = rawGeminiResponseJson.Substring(0, matchJsonBlock.Index).Trim();
        }
        else
        {
            int lastBraceIndex = rawGeminiResponseJson.LastIndexOf('}');
            if (lastBraceIndex != -1)
            {
                int firstBraceIndex = rawGeminiResponseJson.LastIndexOf('{', lastBraceIndex);
                if (firstBraceIndex != -1)
                {
                    string potentialJson = rawGeminiResponseJson.Substring(firstBraceIndex);
                    try
                    {
                        JObject.Parse(potentialJson); // Validate JSON
                        jsonResponse = potentialJson;
                        thoughtsChain = rawGeminiResponseJson.Substring(0, firstBraceIndex).Trim();
                    }
                    catch (JsonReaderException) { /* Invalid block, ignored */ }
                }
            }
        }

        var extracted = Api.Call($"{Program.Module.Domain}/940/Extract/{WebUtility.UrlEncode(jsonResponse)}");
        var messageParts = (extracted as List&lt;string&gt;)
                .Select(segment =&gt; new MessagePart { Text = segment + "\n" })
                .ToList();

        // --- 6. UPDATE CENTRALIZED HISTORY SERVICE ---

        // A. Add the User message to the system service
        Api.Call("System.Utility/ChatHistory/Add", new {
            StoreId = storeId,
            Message = userMessage
        });
        // B. Create the Model response message using the interleaved parts [text, api-call, text...]
        var modelResponse = new ChatMessage
        {
            Role = "model",
            Parts = messageParts
        };
        // C. Add the Model response to the system service
        // The service automatically handles persistence (saving to JSON)
        Api.Call("System.Utility/ChatHistory/Add", new {
            StoreId = storeId,
            Message = modelResponse
        });

        // 7. Return the friendly answer to the UI
        return new ResponseText(String.Join('\n', (extracted as List&lt;string&gt;)));
    }
    catch (Exception ex)
    {
        Program.Notify($"Error while calling Gemini API: {ex.Message}");
        return new ResponseText($"**Error:** An exception occurred while contacting the AI service. Please check the logs.\n\n`{ex.Message}`");
    }
});

// ===================================================================
// API HANDLER: Get the current chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_GET}", (p) =&gt;
{
    return Api.Call("System.Utility/ChatHistory/Get", new { StoreId = Program.Module.Address });
});

// ===================================================================
// API HANDLER: Clear the chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_CLEAR}", (p) =&gt;
{
    var storeId = Program.Module.Address;
    Program.Log.Info($"Clearing history for module {storeId}...");
    return Api.Call("System.Utility/ChatHistory/Clear", new { StoreId = storeId });
});

// Run the program in the background
while (Program.IsRunning &amp;&amp; !reconfigureRequired)
{
    Pause(1);
}
</ScriptSource>
    <ScriptContext>#using System.Text.RegularExpressions

const string API_URL = "AI.IntentHandlers/Gemini";

const string API_PROMPT_SUBMIT = "Prompt.Submit";
const string API_HISTORY_GET = "History.Get";
const string API_HISTORY_CLEAR = "History.Clear";

const string OPTION_API_KEY = "ApiKey";
const string OPTION_MODEL_ID = "ModelId";
const string OPTION_SEND_CONTEXT_DATA = "SendContextData";
const string OPTION_SYSTEM_INSTRUCTIONS = "SystemInstructions";

const string DEFAULT_MODEL_ID = "gemini-2.5-flash-lite";


// POJO

public class MessagePart {
    [JsonProperty("text")]
    public string Text { get; set; }
}
public class ChatMessage {
    [JsonProperty("role")]
    public string Role { get; set; }
    [JsonProperty("parts")]
    public List&lt;MessagePart&gt; Parts { get; set; } = new List&lt;MessagePart&gt;();
    public static ChatMessage Create(string role, string text) {
        return new ChatMessage { Role = role, Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = text } } };
    }
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>gemini-intent-handler</Id>
      <Version>1.0.9</Version>
      <Required>true</Required>
      <Checksum>A5FF200261E243A1CFAD464578344DD9</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>930</Address>
    <Name>Gemini Intent Handler</Name>
    <Description>Enables natural language control for your smart system using Google's Gemini AI.
To activate this feature, enter a valid API key.
Follow the instructions at https://aistudio.google.com/app/apikey to get a free API key.</Description>
    <Group>AI - Machine Learning</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>true</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>// Path to the external markdown file for system instructions
var systemPromptFile = Path.Combine(Data.GetFolder(), "gemini_system_prompt.txt");

// This method is used to register this program as a "AI intent handler"
Program.Implements(
    "@AI:WidgetGenie",
    API_URL
).AddOption("ApiKey", "?", "Google Gemini AI API key", "text")
 .AddOption("ModelId", DEFAULT_MODEL_ID, "Model ID", "text")
 .AddOption("SystemInstructions", "", "System Instructions", $"editor:file:{systemPromptFile}:markdown")
 .Run();
</ScriptSetup>
    <ScriptSource>// ===================================================================
// GEMINI INTENT HANDLER - UPDATED FOR CENTRALIZED HISTORY SERVICE
// ===================================================================
// 1. Validate configuration
string geminiApiKey = Program.Option("ApiKey").Value.Trim();
string geminiModelId = Program.Option("ModelId").Value.Trim();
if (geminiApiKey.Length != 39 || !geminiApiKey.StartsWith("AIzaSy") || string.IsNullOrEmpty(geminiModelId))
{
    string error = "Gemini Intent Handler is not properly configured.";
    //Program.Notify($"ERROR: {error} Please configure the program options.");
    Program.Emit("Status.Init", "NotConfigured");
}
else
{
    Program.Emit("Status.Init", "Ready");
}

bool reconfigureRequired = false;
// Register event handler to trigger a restart if configuration options change.
When.ModuleParameterChanged((module, parameter) =&gt; {
  if (module.Instance == Program.Module &amp;&amp; parameter.Name.StartsWith("ConfigureOptions.")) {
    reconfigureRequired = true;
  }
  return true;
});

// Helper to get the current StoreId (unique for this module)
string storeId = Program.Module.Address;

// ===================================================================
// API HANDLER: Process a new user message
// ===================================================================
Api.Handle($"{API_URL}/{API_PROCESS}", (userCommand) =&gt;
{

    // 2. Fetch the current history from the centralized System Service
    var storeId = Program.Module.Address;
    var result = Api.Call("System.Utility/ChatHistory/Get", new { StoreId = storeId });
    var payloadContents = JArray.FromObject(result).ToObject&lt;List&lt;ChatMessage&gt;&gt;();

    // 3. Prepare the new user message
    var userMessage = ChatMessage.Create("user", userCommand as string);

    // 4. Add User Message to the centralized history immediately
    Api.Call("System.Utility/ChatHistory/Add", new {
        StoreId = storeId,
        Message = userMessage
    });

    // 5. Prepare the System Prompt
    var systemPromptFile = Path.Combine(Data.GetFolder(), "gemini_system_prompt.txt");
    var promptText = "";
    try { promptText = File.ReadAllText(systemPromptFile); } catch { promptText = "You are a helpful home automation assistant."; }

    // 6. Build the API Request Payload (History + New Message)
    payloadContents.Add(userMessage);

    var payload = new
    {
        contents = payloadContents,
        system_instruction = new { parts = new[] { new { text = promptText } } },
        generationConfig = new { temperature = 0.3 }
    };

    try
    {
        // 7. Send the request to Gemini API
        string payloadJson = JsonConvert.SerializeObject(payload);
        string geminiRequestUrl = $"https://generativelanguage.googleapis.com/v1beta/models/{geminiModelId}:generateContent?key={geminiApiKey}";

        var data = Net
            .WebService(geminiRequestUrl)
            .WithTimeout(120)
            .AddHeader("Content-Type", "application/json")
            .Post(payloadJson)
            .GetData();

        // 8. Process the AI response
        string geminiResponse = data?.candidates?[0]?.content?.parts?[0]?.text;

        if (!string.IsNullOrEmpty(geminiResponse))
        {
            // 9. Add Model Response to the centralized history
            var modelResponse = ChatMessage.Create("model", geminiResponse);
            Api.Call("System.Utility/ChatHistory/Add", new {
                StoreId = storeId,
                Message = modelResponse
            });

            return new ResponseText(geminiResponse);
        }
        else
        {
            Program.Log.Warn("Gemini returned an empty response.");
            return new ResponseText("**Error:** The AI returned an empty response.");
        }
    }
    catch (Exception ex)
    {
        Program.Notify($"Gemini API Error: {ex.Message}");
        return new ResponseText($"**Error:** Could not contact Gemini service. `{ex.Message}`");
    }
});

// ===================================================================
// API HANDLER: Get the current chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_GET}", (p) =&gt;
{
    return Api.Call("System.Utility/ChatHistory/Get", new { StoreId = Program.Module.Address });
});

// ===================================================================
// API HANDLER: Clear the chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_CLEAR}", (p) =&gt;
{
    var storeId = Program.Module.Address;
    Program.Log.Info($"Clearing history for module {storeId}...");
    return Api.Call("System.Utility/ChatHistory/Clear", new { StoreId = storeId });
});

// Run in background to keep API endpoints active
while (Program.IsRunning &amp;&amp; !reconfigureRequired)
{
    Pause(1);
}
</ScriptSource>
    <ScriptContext>const string API_URL = "AI.WidgetGenie/Gemini";
const string DEFAULT_MODEL_ID = "gemini-flash-latest";

const string API_PROCESS = "Prompt.Submit";
const string API_HISTORY_GET = "History.Get";
const string API_HISTORY_CLEAR = "History.Clear";

// POJO

public class MessagePart {
    [JsonProperty("text")]
    public string Text { get; set; }
}
public class ChatMessage {
    [JsonProperty("role")]
    public string Role { get; set; }
    [JsonProperty("parts")]
    public List&lt;MessagePart&gt; Parts { get; set; } = new List&lt;MessagePart&gt;();
    public static ChatMessage Create(string role, string text) {
        return new ChatMessage { Role = role, Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = text } } };
    }
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>gemini-widget-genie</Id>
      <Version>1.0.6</Version>
      <Required>true</Required>
      <Checksum>3F9DBFDC62D1F2946F131F1BD7EB23C2</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>931</Address>
    <Name>Gemini Widget Genie</Name>
    <Description>Your personal AI genie for widget creation. Powered by Gemini, it helps you write and debug HTML, CSS, and JavaScript code directly within the HomeGenie editor.
To activate this feature, enter a valid API key.
Follow the instructions at https://aistudio.google.com/app/apikey to get a free API key.</Description>
    <Group>AI - Machine Learning</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>Program
  .AddFeature(
    "",
    ForCameraInputType,
    InstanceSegmentation,
    "Enable instance segmentation",
    "checkbox"
  ).AddFeature(
    "",
    ForCameraInputType,
    InstanceSegmentationTrigger,
    "Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)",
    "text"
  ).AddOption(
    "Yolo.ModelPath",
    Data.GetFolder() + "/yolo26n-seg.onnx",
    "1. Path of YOLO model file (.onnx)",
    "text"
  ).AddOption(
    YoloRequestsPerSecond,
    "5",
    "2. Max Requests Per Second (RPS)",
    "slider:1:30:1"
  );

Program.Run();
</ScriptSetup>
    <ScriptSource>var inputModules = Modules.WithFeature(InstanceSegmentation);
var requestsPerSecond = Program.Option(YoloRequestsPerSecond);

// Show toggle icon for this algorithm in the NVR / Video widget
Program
    .Emit("NVR.Vision.Algorithm", InstanceSegmentation)     // feature to toggle "On" in order to enable this algo
    .Emit("NVR.Vision.Algorithm.Icon", "pattern");          // material symbol icon for the toggle button

// Restart program if configuration has been changed to apply new settings.
When.ModuleParameterChanged( (module, property) =&gt; {
    if (module.Instance == Program.Module &amp;&amp; property.Name.StartsWith("ConfigureOptions."))
    {
        if (Program.IsRunning) Program.Restart();
        return true;
    }
    return true;
});

var yoloModelPath = Program.Option("Yolo.ModelPath")?.Value;
if (String.IsNullOrEmpty(yoloModelPath))
{
    Program.Notify($"Configure the path of YOLO 'segment' model (.onnx file). {OptionButtons}");
    Pause(5);
    return;
}


// For details about this implemention see
// *ML.net* and *YoloSharp* documentation


try
{
    var errorOccurred = false;
    using var detectPredictor = new YoloPredictor(yoloModelPath);
    while (Program.IsRunning)
    {
        if (inputModules.SelectedModules.Count == 0)
        {
            Pause(1);
            continue;
        }
        inputModules.Command("Camera.GetPicture").Submit((m, data) =&gt; {
            if (data == null || ((byte[])data).Length == 0)
            {
                // I/O errors might occasionally occur and can be ignored
                return;
            }
            try
            {
                var result = detectPredictor.Segment((byte[])data);
                //Console.WriteLine($"Result: {result}");
                //Console.WriteLine($"Speed:  {result.Speed}");
                var module = Modules.InDomain(m.Domain).WithAddress(m.Address).Get();
                if (result.Count() &gt; 0)
                {
                    var output = new List&lt;SegmentResult&gt;();
                    // Emit "Sensor.ObjectDetect.Subject.Data" event if anything
                    // mathing the configured "TriggerDetect" list
                    // was detected in the scene.
                    string[] matchList = module
                        .Parameter( InstanceSegmentationTrigger )?.Value
                        .Split(',').Select(p =&gt; p.Trim())
                        .Where(x =&gt; !string.IsNullOrEmpty(x))
                        .ToArray();
                    if (matchList.Length &gt; 0)
                    {
                        var filtered = new List&lt;Compunet.YoloSharp.Data.Segmentation&gt;();
                        foreach (var r in result)
                        {
                            var subject = r.Name.Name;
                            if (matchList.Contains(subject))
                            {
                                filtered.Add(r);
                            }
                        }
                        if (filtered.Count &gt; 0)
                        {
                            if (module.Parameter(ObjectDetect).DecimalValue != filtered.Count)
                            {
                                module.Emit(ObjectDetect, filtered.Count);
                            }
                            else
                            {
                                // update value and timestamp only, do not emit event
                                module.Parameter(ObjectDetect).SetData(filtered.Count);
                            }
                            foreach (var r in filtered)
                            {
                                output.Add(new SegmentResult(){
                                    Result = r,
                                    Mask = ExtractAndApproximateContour(r.Mask, 100)
                                });
                                module.Emit(ObjectDetectSubject, r);
                            }
                        }
                        else if (module.Parameter(ObjectDetect).DecimalValue != 0 &amp;&amp; module.Parameter(ObjectDetect).IdleTime &gt; 5)
                        {
                            module.Emit(ObjectDetect, 0);
                        }
                    }
                    else
                    {
                        foreach (var r in result)
                        {
                            output.Add(new SegmentResult(){
                                Result = r,
                                Mask = ExtractAndApproximateContour(r.Mask, 100)
                            });
                        }
                    }

                    if (output.Count &gt; 0)
                    {
                        // Emit json data for the video player (overlay data)
                        var jsonResults = JsonConvert.SerializeObject(output);
                        module.Emit(VideoPlayerWidgetOverlaySegment, jsonResults);
                    }

                }
            }
            catch (Exception e)
            {
                errorOccurred = true;
                //Console.WriteLine(e.Message);
            }
        });
        if (errorOccurred)
        {
            errorOccurred = false;
            Pause(3);
        }
        else
        {
            if (requestsPerSecond.DecimalValue &gt; 0)
            {
                Pause(1D / requestsPerSecond.DecimalValue);
            }
            else
            {
                Pause(0.2); // default is 5 RPS max
            }
        }
    }
}
catch (Exception e)
{
    Program.Notify($"Error: {e.Message} {OptionButtons}");
    Pause(5);
    return;
}
</ScriptSource>
    <ScriptContext>#using Compunet.YoloSharp.Memory

const string
OptionButtons = "[program_configure,program_disable]",
YoloModelPath = "Yolo.ModelPath",
YoloRequestsPerSecond = "Yolo.RequestsPerSecond",
ForCameraInputType = "VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/",
InstanceSegmentation = "ML.InstanceSegmentation",
InstanceSegmentationTrigger = $"{InstanceSegmentation}.TriggerDetect",
ObjectDetect = "Sensor.ObjectDetect",
ObjectDetectSubject = "Sensor.ObjectDetect.Subject",
VideoPlayerWidgetOverlaySegment = "Widget.Data.VideoPlayer.Overlay.Segment";

public class SegmentResult
{
    public Compunet.YoloSharp.Data.Segmentation Result;
    public List&lt;MaskPoint&gt; Mask;
}

public class MaskPoint
{
    public float X;
    public float Y;
    public MaskPoint(float x, float y)
    {
        X = x;
        Y = y;
    }
}

public static List&lt;MaskPoint&gt; ExtractAndApproximateContour(BitmapBuffer r, int maxPoints = 25)
{
    var contour = ExtractContourPointsInternal(r);  // Get the initial contour

    if (contour.Count &lt;= maxPoints)
    {
        return contour; // No need to approximate if already within the limit
    }

    // Implement a simple approximation by taking equally spaced points
    var approximatedContour = new List&lt;MaskPoint&gt;();
    double interval = (double)contour.Count / maxPoints;
    for (int i = 0; i &lt; maxPoints - 1; i++)
    {
        int index = (int)Math.Round(i * interval); //Round for accuracy
        approximatedContour.Add(contour[index]); // Add existing point at the index
    }

    //Ensure the loop has the final item; If the last isn't added, add it
    if(approximatedContour.Last() != contour.Last()) {
            approximatedContour.Add(contour.Last());
    }

    return approximatedContour;
}


private static List&lt;MaskPoint&gt; ExtractContourPointsInternal(BitmapBuffer r)
{
    int height = r.Height;
    int width = r.Width;

    // Helper function to check if a pixel is a border pixel
    bool IsBorderPixel(BitmapBuffer mask, int x, int y)
    {
        if (x == 0 || x == width - 1 || y == 0 || y == height - 1)
        {
            return true;
        }

        return (mask[y, x - 1] &gt; 0.9 != mask[y, x] &gt; 0.9 ||
                mask[y, x + 1] &gt; 0.9 != mask[y, x] &gt; 0.9 ||
                mask[y - 1, x] &gt; 0.9 != mask[y, x] &gt; 0.9 ||
                mask[y + 1, x] &gt; 0.9 != mask[y, x] &gt; 0.9);
    }

    // Find all contour pixels
    var contourPixels = new List&lt;MaskPoint&gt;();
    for (int y = 0; y &lt; height; y++)
    {
        for (int x = 0; x &lt; width; x++)
        {
            if (r[y, x] &gt; 0.9 &amp;&amp; IsBorderPixel(r, x, y))
            {
                contourPixels.Add(new MaskPoint(x, y));
            }
        }
    }

    if (contourPixels.Count == 0)
    {
        return new List&lt;MaskPoint&gt;(); // Return empty list if no shape found
    }

    // Order the contour pixels (nearest neighbor approach)
    var contour = new List&lt;MaskPoint&gt;();
    var startPixel = contourPixels[0];
    contour.Add(startPixel);
    contourPixels.Remove(startPixel);

    var currentPixel = startPixel;

    while (contourPixels.Count &gt; 0)
    {
        var nearestNeighbor = contourPixels
            .OrderBy(p =&gt; Math.Sqrt(Math.Pow(p.X - currentPixel.X, 2) + Math.Pow(p.Y - currentPixel.Y, 2))) // Euclidean distance
            .FirstOrDefault();

        if (nearestNeighbor == null)  // No more neighbors found
        {
            break;
        }

        contour.Add(nearestNeighbor);
        contourPixels.Remove(nearestNeighbor);
        currentPixel = nearestNeighbor;
    }

    return contour;
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>instance-segmentation</Id>
      <Version>1.0.3</Version>
      <Required>true</Required>
      <Checksum>AD92A3DD40DDDFBFE4A69385C64AF26A</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>912</Address>
    <Name>Instance Segmentation</Name>
    <Description>Detect objects and their contour mask using a pre-trained YOLO model.
</Description>
    <Group>AI - Machine Learning</Group>
    <Features>
      <ProgramFeature>
        <FieldType>checkbox</FieldType>
        <ForDomains />
        <ForTypes>VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/</ForTypes>
        <Property>ML.InstanceSegmentation</Property>
        <Description>Enable instance segmentation</Description>
      </ProgramFeature>
      <ProgramFeature>
        <FieldType>text</FieldType>
        <ForDomains />
        <ForTypes>VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/</ForTypes>
        <Property>ML.InstanceSegmentation.TriggerDetect</Property>
        <Description>Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)</Description>
      </ProgramFeature>
    </Features>
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>// ===================================================================
// SETUP BLOCK - PROGRAM INITIALIZATION
// ===================================================================
// This block is executed once when the program is enabled.
// It is responsible for:
// 1. Loading the available models for the UI selection.
// 2. Registering the program as a system-wide AI Intent Handler.
// 3. Defining user-configurable options (tokens, model, prompt).
// 4. Linking the specific front-end chat widget.
// ===================================================================

// --- 1. PRE-SETUP CONFIGURATION DATA ---

// Load available GGUF models from models.yaml to populate the dropdown
var modelsList = LoadModelList(Path.Combine(Data.GetFolder(), CONFIG_FILE_NAME));
var models = string.Join(",", modelsList.Select(item =&gt; $"{item.Value.Name}={item.Key}"));

// Path to the external markdown file for system instructions
var systemPromptFile = Path.Combine(Data.GetFolder(), "system_prompt.txt");
var modelsFile = Path.Combine(Data.GetFolder(), CONFIG_FILE_NAME);


// --- 2. PROGRAM SETUP DELEGATE ---

Program.Setup(() =&gt; {

  Program
    // Implement the AI Intent Handler interface to allow system integration
    .Implements(
      "@AI:IntentHandler",
      API_URL
    )

    // UI Options Definition:
    // - Response length limit (Max Tokens)
    // - Model selection dropdown (populated from models.yaml)
    // - System Instructions (Markdown editor linked to system_prompt.txt)
    .AddOption(OPTION_MODEL_FILE, "", "0. AI Model", $"select:{ models }")
    .AddOption(OPTION_SEND_CONTEXT_DATA, "On", "1. Share system and module context with the AI", "checkbox")
    .AddOption(OPTION_MODEL_TEMPERATURE, "0.4", "2. Creativity (0.0 = Precise, 2.0 = Creative)", "slider:0.0:2.0:0.05")
    .AddOption(OPTION_CONTEXT_SIZE, "2048", "3. AI Memory Capacity (Context Window)", "slider:1024:32768:1024")
    .AddOption(OPTION_MAX_TOKENS, "1536", "4. Response Length (Max Tokens)", "slider:64:4096:32")
    .AddOption(OPTION_MAX_TURNS, "10", "5. Chat Memory (Max Turns) (0 = no limit)", "slider:0:20:1")
    .AddOption(OPTION_SYSTEM_INSTRUCTIONS, "", "6. System Prompt (Instructions)", $"editor:file:{systemPromptFile}:markdown")
    .AddOption(OPTION_MODEL_LIST, "", "7. AI Model List", $"editor:file:{modelsFile}:yaml")

    // Assign the custom UI widget for the chat interface
    .UseWidget("custom/examples/local-ai-chat")
    .Emit("@AI:IntentHandler", API_URL); // notify widgets

});


// --- 3. EXECUTION ---

// Trigger the transition to the MAIN block
Program.Run();
</ScriptSetup>
    <ScriptSource>// ===================================================================
// LOCAL LLM AUTOMATION PROGRAM (LLamaSharp Edition)
// ===================================================================
// This block manages the integration of a Local Language Model (GGUF)
// for home automation control within HomeGenie.
//
// Main Responsibilities:
// 1. Model download management via System DownloadManager.
// 2. LLamaSharp engine initialization (VRAM/RAM management).
// 3. Dynamic device inventory generation for the System Prompt.
// 4. Token streaming with smart buffering and artifact filtering.
// 5. Automatic execution of API commands extracted from AI responses.
// ===================================================================

// --- 1. CONFIGURATION AND STATE INITIALIZATION ---

var reconfigureRequired = false;
int maxMessages = (int)Program.Option(OPTION_MAX_TURNS).DecimalValue * 2;
var modelsFile = Path.Combine(Data.GetFolder(), CONFIG_FILE_NAME);

// Load model configurations and file paths
var modelsList = LoadModelList(modelsFile);
var selectedModel = Program.Option(OPTION_MODEL_FILE).Value;
string localModelFile = Path.Combine(Data.GetFolder(), selectedModel);



// Register event handler to trigger a restart if configuration options change.
When.ModuleParameterChanged((module, parameter) =&gt; {
  if (module.Instance == Program.Module &amp;&amp; parameter.Name.StartsWith("ConfigureOptions.")) {
    if (parameter.Name.EndsWith($".{OPTION_MODEL_LIST}"))
    {
      // Update model list option in the UI
      modelsList = LoadModelList(modelsFile);
      var models = string.Join(",", modelsList.Select(item =&gt; $"{item.Value.Name}={item.Key}"));
      models = $"select:{ models }";
      // IMPORTANT: update only if changed to prevent loop Emit &lt;-&gt; ModuleParameterChanged
      if (parameter.Value != models)
      {
        Program.Option(OPTION_MODEL_FILE).FieldType = models;
      }
    }
    else
    {
      reconfigureRequired = true;
    }
  }
  return true;
});



// Reset UI status fields
Program
  .Emit("LLM.Id", "")
  .Emit("Status.System.RamGb", GetTotalMemoryGB())
  .Emit(FIELD_STATUS_INIT, "NotConfigured");


// --- 2. API: DOWNLOAD MANAGEMENT ---
// These endpoints bridge the UI with the system's DownloadManager utility.

Api.Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadStart", (p) =&gt; {
     var model = modelsList[selectedModel];
     Program.Emit(FIELD_STATUS_INIT, "Download").Emit(FIELD_DOWNLOAD, "Requested");
     var item = JsonConvert.SerializeObject(new {
         Name = model.Name,
         Url = model.Url,
         Path = Data.GetFolder()
     });
     Api.Call("System.Utility/DownloadManager/Enqueue", item);
     return new ResponseStatus(Status.Ok);

}).Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadCancel", (p) =&gt; {
     Program.Emit(FIELD_DOWNLOAD, "Pending");
     var item = JsonConvert.SerializeObject(new { Id = Program.Parameter(FIELD_DOWNLOAD_ID)?.Value });
     Api.Call("System.Utility/DownloadManager/Delete", item);
     Program
      .Emit(FIELD_DOWNLOAD_ID, "")
      .Emit(Program.Option(OPTION_MODEL_FILE).Name, "");
     return new ResponseStatus(Status.Ok);

}).Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadPause", (p) =&gt; {
    Program.Emit(FIELD_DOWNLOAD, "Paused");
    var item = JsonConvert.SerializeObject(new { Id = Program.Parameter(FIELD_DOWNLOAD_ID)?.Value });
    Api.Call("System.Utility/DownloadManager/Pause", item);
    return new ResponseStatus(Status.Ok);

}).Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadResume", (p) =&gt; {
    var item = JsonConvert.SerializeObject(new { Id = Program.Parameter(FIELD_DOWNLOAD_ID)?.Value });
    Api.Call("System.Utility/DownloadManager/Resume", item);
    return new ResponseStatus(Status.Ok);
});


// --- 2.1 INFERENCE POST-PROCESSING HELPER METHOD (shared for using by other programs)
Api.Handle($"{Program.Module.Domain}/{Program.Module.Address}/Extract", (modelResponse) =&gt; {
    // Post-Process AI reply and execute local commands
    var request = Api.Parse(modelResponse);
    var response = WebUtility.UrlDecode(request.OriginalRequest);
    return ExtractAndExecute(Api, response);
});


// --- 3. MODEL FILE VERIFICATION &amp; MONITORING LOOP ---

if (!File.Exists(localModelFile)) {
  // Model file missing: enter monitoring loop until download is finished.
  while (!File.Exists(localModelFile))
  {
    dynamic response = Api.Call("System.Utility/DownloadManager/GetStatus", localModelFile);
    dynamic fileStatus = JsonConvert.DeserializeObject&lt;dynamic&gt;(response);

    if ((bool)fileStatus?.Success)
    {
      var task = fileStatus?.Data;
      if (task != null)
      {
        // Update UI with real-time download progress
        Program
          .Emit(FIELD_DOWNLOAD, task.Status.ToString())
          .Emit(FIELD_DOWNLOAD_ID, task.Id.ToString())
          .Emit(FIELD_DOWNLOAD_TASK, task.DestinationFilePath.ToString())
          .Emit(FIELD_DOWNLOAD_ERROR, task.ErrorMessage.ToString())
          .Emit(FIELD_DOWNLOAD_PROGRESS, task.ProgressPercentage.ToString("F2", CultureInfo.InvariantCulture));

        if (task.ProgressPercentage &gt;= 100) break;
      }
      else if (Program.Parameter(FIELD_DOWNLOAD_TASK)?.Value != localModelFile)
      {
        // No active task found for this file
        Program
          .Emit(FIELD_DOWNLOAD, "Pending")
          .Emit(FIELD_DOWNLOAD_ID, String.IsNullOrEmpty(selectedModel) ? "Pending" : "")
          .Emit(FIELD_DOWNLOAD_TASK, localModelFile)
          .Emit(FIELD_DOWNLOAD_PROGRESS, "0");
          if (modelsList.ContainsKey(selectedModel))
          {
            Program.Emit(FIELD_DOWNLOAD_REPOSITORY, modelsList[selectedModel].Repository);
          }
          else
          {
            Program.Emit(FIELD_DOWNLOAD_REPOSITORY, "");
          }
      }
    }

    if (reconfigureRequired) return;
    Pause(2); // Poll status every 2 seconds
  }

  Program
    .Emit(FIELD_STATUS_ERROR, "")
    .Emit(FIELD_DOWNLOAD, "Complete")
    .Emit(FIELD_DOWNLOAD_PROGRESS, "100");

  return; // Program will restart to initialize the engine with the new file
}
else if (Program.Parameter(FIELD_DOWNLOAD_TASK)?.Value != localModelFile)
{
  // File already exists; ensure UI state is consistent
  Program
    .Emit(FIELD_DOWNLOAD, "Complete")
    .Emit(FIELD_DOWNLOAD_ID, "")
    .Emit(FIELD_DOWNLOAD_PROGRESS, "100")
    .Emit(FIELD_DOWNLOAD_TASK, localModelFile);
}


// --- 4. LLM ENGINE INITIALIZATION (LLamaSharp) ---

Program
  .Emit("LLM.Id", selectedModel)
  .Emit(FIELD_STATUS_ERROR, "")
  .Emit(FIELD_STATUS_INIT, $"Starting");

var templateType = "";
var modelPath = localModelFile;

if (modelsList.ContainsKey(selectedModel)) {
  templateType = modelsList[selectedModel].Template;
} else {
  // ERROR: selected model not listed
  modelPath = "";
  Program
    .Emit(FIELD_DOWNLOAD_ID, "")
    .Emit(Program.Option(OPTION_MODEL_FILE).Name, "");
}

LLamaWeights model = null;
LLamaContext context = null;
ChatSession session = null;
CancellationTokenSource generationCts = null;

// Cleanup action to release unmanaged resources (VRAM/RAM)
var dispose = new Action(() =&gt; {
  session = null;
  if (context != null) { context.Dispose(); context = null; }
  if (model != null) { model.Dispose(); model = null; }
  GC.Collect();
  GC.WaitForPendingFinalizers();
});

// Basic path validation
if (String.IsNullOrEmpty(modelPath))
{
  Program.Notify($"LLM model path not configured.[program_configure]").Emit(FIELD_STATUS_ERROR, "LLM model path not configured.");
}
else if (!File.Exists(modelPath))
{
  Program.Notify($"File not found: {modelPath}.[program_configure]").Emit(FIELD_STATUS_ERROR, $"File not found: {modelPath}");
}
else
{
  // Ensure cleanup on stop
  When.ProgramStopping(() =&gt; { dispose(); return true; });

  try {
    // Load Inference Parameters
    LailamaInferenceParams.MaxTokens = (int)Program.Option(OPTION_MAX_TOKENS).DecimalValue;
    uint contextSize = (uint)Program.Option(OPTION_CONTEXT_SIZE).DecimalValue;

    var parameters = GetOptimizedParams(modelPath);

    // Load Model Weights and Context
    model = LLamaWeights.LoadFromFile(parameters);
    context = model.CreateContext(parameters);

    var executor = new InteractiveExecutor(context);
    var chatHistory = new ChatHistory();

    // --- 5. SYSTEM PROMPT CONFIGURATION AND CHAT SESSION INIT ---
    var promptText = "";
    try { promptText = File.ReadAllText(Path.Combine(Data.GetFolder(), "system_prompt.txt")); } catch (Exception e) { }

    var systemPrompt = (string)Api.Call("System.Utility/ContextEngine/Render", promptText);

    // Clean initialization
    var systemMessage = new ChatHistory.Message(AuthorRole.System, (string)systemPrompt);
    chatHistory.Messages.Add(systemMessage);


    // SYNC: Fetch clean history from the System Service and sync LLamaSharp session
    string storeId = Program.Module.Address;
    try {
        var result = Api.Call("System.Utility/ChatHistory/Get", new { StoreId = storeId });
        dynamic unifiedlHistory = JArray.FromObject(result).ToObject&lt;List&lt;ChatMessage&gt;&gt;();

        if (unifiedlHistory != null) {

            // Re-populate with clean messages from service
            foreach(ChatMessage msg in unifiedlHistory) {
                var role = msg.Role == "user" ? AuthorRole.User : AuthorRole.Assistant;

                // --- FIX: Uso di un ciclo classico invece di LINQ .Select() ---
                var partsTextList = new List&lt;string&gt;();
                foreach (var part in msg.Parts) {
                    partsTextList.Add(part.Text);
                }
                string combinedText = string.Join("\n", partsTextList);
                chatHistory.Messages.Add(new ChatHistory.Message(role, combinedText));
            }
        }
    } catch (Exception ex) {
        Program.Log.Error("Sync error: " + ex.Message);
    }


    // Final session starts with the System Prompt already in history
    session = new ChatSession(executor, chatHistory);

    var resetState = session.GetSessionState();
    session.LoadSession(resetState);

    // --- 6. WARMUP PHASE ---
    // Perform a dummy inference to pre-populate the KV cache and avoid latency on first use.
    Program.Emit(FIELD_STATUS_INIT, "WarmingUp");
    try {
        var preWarmupState = session.GetSessionState();
        var warmupParams = new InferenceParams() { MaxTokens = 1 };
        Task.Run(async () =&gt; {
            await foreach (var token in session.ChatAsync(new ChatHistory.Message(AuthorRole.User, " "), warmupParams)) { }
        }).GetAwaiter().GetResult();
        session.LoadSession(preWarmupState);
    } catch (Exception ex) {
        Program.Log.Warn($"Warmup warning: {ex.Message}");
    }
    Program.Emit(FIELD_STATUS_INIT, "Ready");


    // --- 7. INFERENCE LOGIC AND TOKEN PROCESSING ---
    /**
      * Processes the user input, handles LLM inference, manages the token stream,
      * and synchronizes with the centralized Chat History Service.
      *
      * @param rawInput The clean text entered by the user.
      * @param synchronous Whether to wait for the full response before returning.
      * @return The full generated response (if synchronous), otherwise an empty string.
      */
    var processInput = new Func&lt;string, bool, bool, bool, string&gt;((rawInput, synchronous, sendContextData, noHistory) =&gt; {

        if (generationCts != null) generationCts.Dispose();
        generationCts = new CancellationTokenSource();

        string fullInput = rawInput;
        if (sendContextData)
        {
          string systemInfo = (string)Api.Call("System.Utility/ContextEngine/Render", "%%SYSTEM_STATUS%%");
          fullInput = systemInfo + "\n\n" + rawInput;
        }


        // 2. PERSIST USER MESSAGE: Save the CLEAN raw input to the service
        if (!noHistory)
        {
          Api.Call("System.Utility/ChatHistory/Add", new {
              StoreId = storeId,
              Message = ChatMessage.Create("user", rawInput)
          });
        }

        // 3. TEMPLATING: Prepare input for the engine (but don't save this to history!)
        string templatedInput = BuildUserPrompt(templateType, fullInput);

        var process = new Func&lt;bool, string&gt;((sync) =&gt; {
            // Send the templated input to the engine
            var sessionChat = session.ChatAsync(
                new ChatHistory.Message(AuthorRole.User, templatedInput),
                LailamaInferenceParams,
                cancellationToken: generationCts.Token
            );

            string streamBuffer = "";
            string fullReply = "";
            var technicalTags = LailamaInferenceParams.AntiPrompts.Where(x =&gt; x.StartsWith("&lt;")).ToArray();
            var textStops = LailamaInferenceParams.AntiPrompts.Where(x =&gt; !x.StartsWith("&lt;")).ToArray();
            string[] visualArtifacts = { "/assistant/avatar", "/avatar", "![avatar]", "![image]" };

            ProcessAsync(sessionChat, (token) =&gt; {
                if (string.IsNullOrEmpty(token)) return;
                streamBuffer += token;
                fullReply += token;

                foreach (var art in visualArtifacts) if (streamBuffer.Contains(art)) streamBuffer = streamBuffer.Replace(art, "");
                foreach (var tag in technicalTags) if (streamBuffer.Contains(tag)) streamBuffer = streamBuffer.Replace(tag, "");
                foreach (var stop in textStops) {
                    if (streamBuffer.EndsWith(stop)) { streamBuffer = ""; return; }
                }

                bool isPotentialDanger = false;
                foreach (var stop in textStops) if (stop.StartsWith(streamBuffer)) { isPotentialDanger = true; break; }

                if (!isPotentialDanger || streamBuffer.Length &gt; 25) {
                    Program.Emit(FIELD_TOKENS_STREAM, streamBuffer);
                    streamBuffer = "";
                }
            }).GetAwaiter().GetResult();

            if (!string.IsNullOrEmpty(streamBuffer)) {
                foreach (var tag in technicalTags) streamBuffer = streamBuffer.Replace(tag, "");
                if (!string.IsNullOrWhiteSpace(streamBuffer)) Program.Emit(FIELD_TOKENS_STREAM, streamBuffer);
            }
            Program.Emit(FIELD_TOKENS_STREAM, "");

            // 4. COMMAND EXECUTION AND PERSISTENCE
            var messageSegments = ExtractAndExecute(Api, fullReply);

            var modelResponse = new ChatMessage {
                Role = "model",
                Parts = messageSegments.Select(s =&gt; new MessagePart { Text = s }).ToList()
            };

            if (!noHistory)
            {
              Api.Call("System.Utility/ChatHistory/Add", new {
                  StoreId = storeId,
                  Message = modelResponse
              });
            }

            if (sendContextData)
            {
              // Replace content with the clean text (without systemInfo)
              var lastUserMsg = session.History.Messages.LastOrDefault(m =&gt; m.AuthorRole == AuthorRole.User);
              if (lastUserMsg != null)
              {
                  lastUserMsg.Content = rawInput;
              }
            }

            // Check chat history (excluding the System Message at index 0)
            // If history size exceeds the limit (System + max allowed messages), start pruning.
            /*
            while (maxMessages &gt; 0 &amp;&amp; session.History.Messages.Count &gt; maxMessages + 1)
            {
                // Remove the oldest message immediately following the System Message.
                // Index 1 represents the earliest User or Assistant message in the conversation.
                session.History.Messages.RemoveAt(1);
            }
            */
            if (session.History.Messages.Count &gt; maxMessages)
            {
              session.LoadSession(resetState);
              session.History.Messages.Clear();
              //context.NativeHandle.MemoryClear(data: true);
              systemPrompt = (string)Api.Call("System.Utility/ContextEngine/Render", promptText);
              systemMessage = new ChatHistory.Message(AuthorRole.System, (string)systemPrompt);
              session.History.Messages.Add(systemMessage);
            }

            return fullReply;
        });

        if (synchronous) return process(true);

        Task.Run(() =&gt; {
            try { process(false); }
            catch (OperationCanceledException) { Program.Log.Info("Inference stopped."); }
            catch (Exception ex) { Program.Emit(FIELD_STATUS_ERROR, ex.Message); }
            finally { generationCts?.Dispose(); generationCts = null; }
        });

        return "";
    });


    // --- 8. API ENDPOINTS: INFERENCE AND CHAT ---

    Api.Handle($"{API_URL}/{API_PROMPT_SUBMIT}", (userInput) =&gt; {
        // Direct intent submission (e.g. from external modules or scripts)
        string input = WebUtility.UrlDecode((string)userInput);
        if (!String.IsNullOrWhiteSpace(input))
        {
            bool sendContextData = (Program.Option(OPTION_SEND_CONTEXT_DATA).Value == "On");
            processInput(input, false, sendContextData, false);
        }
        return new ResponseStatus(Status.Ok, "Processing: " + input);

    }).Handle($"{API_URL}/{API_PROMPT_SCHEDULE}", (command) =&gt; {
        // Direct intent submission (e.g. from external modules or scripts)
        string input = WebUtility.UrlDecode((string)command);
        if (!String.IsNullOrWhiteSpace(input))
        {
          return new ResponseStatus(Status.Ok, processInput(input, true, false, true));
        }
        return new ResponseStatus(Status.Error, "No input");

    }).Handle($"{API_URL}/{API_PROMPT_CANCEL}", (userCommand) =&gt; {
        // Abort current generation
        if (generationCts != null) {
            generationCts.Cancel();
            Program.Emit(FIELD_TOKENS_STREAM, "\n\n[STOPPED]");
        }
        return new ResponseStatus(Status.Ok);

    }).Handle($"{API_URL}/{API_HISTORY_GET}", (userCommand) =&gt; {

      return Api.Call("System.Utility/ChatHistory/Get", new {
          StoreId = Program.Module.Address
      });

    }).Handle($"{API_URL}/{API_HISTORY_CLEAR}", (userCommand) =&gt; {

      var storeId = Program.Module.Address;
      Api.Call("System.Utility/ChatHistory/Clear", new { StoreId = storeId });
      if (session != null &amp;&amp; resetState != null)
      {
          session.LoadSession(resetState);
          if (session.History.Messages.Count &gt; 0)
          {
              session.History.Messages.Clear();
              //context.NativeHandle.MemoryClear(data: true);
              systemPrompt = (string)Api.Call("System.Utility/ContextEngine/Render", promptText);
              systemMessage = new ChatHistory.Message(AuthorRole.System, (string)systemPrompt);
              session.History.Messages.Add(systemMessage);
          }
      }
      Program.Log.Info($"[Module {storeId}] Cleared: Engine and Service are now empty.");
      return new ResponseStatus(Status.Ok);

    });

    Program.Emit(FIELD_STATUS_ERROR, "");

  } catch (Exception e) {

    Program.Notify($"ERROR: {e.Message} [program_configure]").Emit(FIELD_STATUS_ERROR, e.Message);
    Pause(5);

  }
}

Program.Emit(FIELD_TOKENS_STREAM, "");

// --- 9. KEEP-ALIVE AND LIFECYCLE MONITORING ---

while (Program.IsRunning)
{
    if (reconfigureRequired)
    {
        dispose();
        return; // Exiting this loop triggers HomeGenie to reload the program block
    }
    Pause(1);
}
</ScriptSource>
    <ScriptContext>#using System.Text.RegularExpressions

// ===================================================================
// CONFIGURATION AND CONSTANTS
// ===================================================================

/* Internal API Routing */
const string API_URL = "AI.IntentHandlers/Lailama";
const string API_PROMPT_SUBMIT = "Prompt.Submit";
const string API_PROMPT_SCHEDULE = "Prompt.Schedule";
const string API_PROMPT_CANCEL = "Prompt.Cancel";
const string API_HISTORY_GET = "History.Get";
const string API_HISTORY_CLEAR = "History.Clear";

/* Module Option Keys */
const string OPTION_SEND_CONTEXT_DATA = "SendContextData";
const string OPTION_CONTEXT_SIZE = "ContextSize";
const string OPTION_MAX_TOKENS = "MaxTokens";
const string OPTION_MAX_TURNS = "MaxTurns";
const string OPTION_MODEL_FILE = "ModelFile";
const string OPTION_MODEL_LIST = "ModelList";
const string OPTION_MODEL_TEMPERATURE = "ModelTemperature";
const string OPTION_SYSTEM_INSTRUCTIONS = "SystemInstructions";

/* UI and Status Field Names */
const string FIELD_TOKENS_STREAM = "LLM.TokenStream";
const string FIELD_STATUS_INIT = "Status.Init";
const string FIELD_STATUS_ERROR = "Status.Error";
const string FIELD_DOWNLOAD = "Status.Download";
const string FIELD_DOWNLOAD_ID = "Status.Download.Id";
const string FIELD_DOWNLOAD_TASK = "Status.Download.Task";
const string FIELD_DOWNLOAD_REPOSITORY = "Status.Download.Repository";
const string FIELD_DOWNLOAD_ERROR = "Status.Download.Error";
const string FIELD_DOWNLOAD_PROGRESS = "Status.Download.Progress";

/* Resource Settings */
const string CONFIG_FILE_NAME = "models.yaml";
const int PARAMS_GPU_LAYER_SIZE = 0; // Set &gt; 0 to enable GPU offloading (VRAM)

// ===================================================================
// DATA MODELS AND CONFIGURATION LOADING
// ===================================================================

public class ModelInfo {
  public string Url = "";
  public string Name = "";
  public string Repository = "";
  public string Description = "";
  public string Size = "";
  public string Template = "";
}

/**
 * Loads the list of available models from a YAML configuration file.
 */
public static Dictionary&lt;string, ModelInfo&gt; LoadModelList(string configFilePath = CONFIG_FILE_NAME)
{
    string fullPath = configFilePath;
    try
    {
        string yaml = File.ReadAllText(fullPath);
        var deserializer = new DeserializerBuilder().Build();
        return deserializer.Deserialize&lt;Dictionary&lt;string, ModelInfo&gt;&gt;(yaml);
    }
    catch (Exception ex)
    {
        // Return empty dictionary if file is missing or corrupt
        return new Dictionary&lt;string, ModelInfo&gt;();
    }
}

public class MessagePart {
    [JsonProperty("text")]
    public string Text { get; set; }
}
public class ChatMessage {
    [JsonProperty("role")]
    public string Role { get; set; }
    [JsonProperty("parts")]
    public List&lt;MessagePart&gt; Parts { get; set; } = new List&lt;MessagePart&gt;();
    public static ChatMessage Create(string role, string text) {
        return new ChatMessage { Role = role, Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = text } } };
    }
}

// ===================================================================
// INFERENCE PARAMETERS (Sampling &amp; Stop Sequences)
// ===================================================================

InferenceParams LailamaInferenceParams = new InferenceParams
{
    SamplingPipeline = new DefaultSamplingPipeline
    {
        Temperature = 0.4f, // Lower temperature for more deterministic/factual responses
        TopP = 0.95f,
        RepeatPenalty = 1.2f
    },
    MaxTokens = 1024,

    // ANTI-PROMPTS (Stop Sequences)
    // This list prevents the model from "hallucinating" both sides of the conversation
    // or continuing indefinitely after providing the answer.
    AntiPrompts = new string[]
    {
        // --- GROUP 1: OFFICIAL EOS (End Of Sentence) TOKENS ---
        // Internal markers specific to each model architecture.
        "&lt;|im_end|&gt;",           // ChatML (Qwen, Phi-3, Yi)
        "&lt;|endoftext|&gt;",        // GPT-based standards
        "&lt;|eot_id|&gt;",           // Llama 3
        "&lt;|eom_id|&gt;",           // Llama 3 (End of Message)
        "&lt;|end|&gt;",              // Phi-3
        "&lt;end_of_turn&gt;",        // Gemma 2
        "&lt;|end_of_sentence|&gt;",  // DeepSeek
        "&lt;|end_of_response|&gt;",  // DeepSeek R1 / Reasoning models

        // --- GROUP 2: TURN-START MARKERS ---
        // Used to catch the model if it tries to generate the next User/System turn.
        "&lt;|im_start|&gt;",         // ChatML
        "&lt;|start_header_id|&gt;",  // Llama 3
        "&lt;|user|&gt;",             // Phi-3
        "&lt;start_of_turn&gt;",      // Gemma
        "&lt;|begin_of_sentence|&gt;",// DeepSeek
        "&lt;|start_response|&gt;",   // Reasoning response start
        "Answer: ",             // DeepSeek fallback

        // --- GROUP 3: TEXTUAL FALLBACKS ---
        // Prevents the model from writing plain-text labels like "Assistant:" or "User:".
        "Assistant:",
        "\nAssistant:",
        "Response:",
        "\nResponse:",
        "### Assistant",
        "### Response",
        "\nUser:",
        "\nUser ",
        "User:",
        "### Instruction",
        "### User",
        "\nSystem:",
        "System:",
        "\n\nUser:",
        "\n\nSystem:",
        "\n\nAssistant:"
    }
};

// ===================================================================
// PROMPT TEMPLATING LOGIC
// ===================================================================

/**
 * Wraps the system instructions in the format required by the specific model template.
 */
string GetSystemPrompt(string templateType, string prompt) {
  switch (templateType) {
    case "llama3":
      return $"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{prompt}\n&lt;|eot_id|&gt;";

    case "phi3":
      return $"{prompt}\n\n";

    case "gemma":
       return $"&lt;start_of_turn&gt;user\n{prompt}&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\nOK.&lt;end_of_turn&gt;\n";

    case "alpaca":
    case "openchat":
      return $"{prompt}\n\n";

    case "deepseek":
      return $"&lt;|begin_of_sentence|&gt;{prompt}\n\n";

    case "chatml":
    default:
      return $"&lt;|im_start|&gt;system\n{prompt}\n&lt;|im_end|&gt;\n";
  }
}

/**
 * Wraps user input and prepares the assistant's response header based on the template.
 */
string BuildUserPrompt(string templateType, string userInput) {
  switch (templateType) {
    case "llama3":
      return $"&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{userInput}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n";

    case "phi3":
      return $"&lt;|user|&gt;\n{userInput}&lt;|end|&gt;\n&lt;|assistant|&gt;\n";

    case "alpaca":
      return $"### Instruction:\n{userInput}\n\n### Response:\n";

    case "openchat":
      return $"GPT4 Correct User: {userInput}&lt;|end_of_turn|&gt;GPT4 Correct Assistant:";

    case "gemma":
      return "&lt;start_of_turn&gt;user\n" + userInput + "&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n";

    case "deepseek":
      return $"User:{userInput}\n\nAssistant: ";

    case "chatml":
    default:
      return $"&lt;|im_start|&gt;user\n{userInput}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n";
  }
}

// ===================================================================
// UTILITY HELPERS AND CONTENT EXTRACTION
// ===================================================================

/**
 * Helper to process the asynchronous token stream from the LLM.
 */
async Task ProcessAsync(IAsyncEnumerable&lt;string&gt; asyncEnumerable, Action&lt;string&gt; callback)
{
    await foreach (string item in asyncEnumerable)
    {
        callback(item);
    }
}

public class ApiCallBlock
{
    public string Content { get; set; }
    public int StartIndex { get; set; }
    public int EndIndex { get; set; }
}

public class CodeBlockExtractor
{
    // Regex che cattura l'intero blocco comprese le etichette di apertura e chiusura
    private const string ApiCallPattern = @"(```api-call[\s\S]*?```)";

    /**
     * Extracts interleaved text and API blocks.
     * API segments are returned exactly as: ```api-call\n...\n```
     */
    public static List&lt;string&gt; ExtractInterleaved(string sourceText)
    {
        var result = new List&lt;string&gt;();
        if (string.IsNullOrEmpty(sourceText)) return result;

        // Regex.Split con il pattern tra parentesi tonde include i "delimitatori" nel risultato
        var parts = Regex.Split(sourceText, ApiCallPattern, RegexOptions.Singleline);

        foreach (var part in parts)
        {
            if (string.IsNullOrEmpty(part)) continue;

            // Aggiungiamo il segmento alla lista (sia esso testo o blocco api-call)
            result.Add(part.Trim());
        }

        return result;
    }
}

/**
 * Executes API calls and returns the interleaved list [text, ```api-call\n...\n```, text, ...]
 */
public List&lt;string&gt; ExtractAndExecute(dynamic Api, string response)
{
    var segments = CodeBlockExtractor.ExtractInterleaved(response);
    foreach (var segment in segments)
    {
        if (segment.StartsWith("```api-call"))
        {
            string commandsRaw = segment
                .Replace("```api-call", "")
                .Replace("```", "")
                .Trim();

            var commands = commandsRaw.Split(new[] { '\n', '\r' }, StringSplitOptions.RemoveEmptyEntries);
            foreach(var cmd in commands)
            {
                string cleanCmd = cmd.Trim();
                if (!string.IsNullOrEmpty(cleanCmd))
                {
                    Program.Log.Info($"Executing: {cleanCmd}");
                    try {
                        Api.Call($"/api/{cleanCmd}");
                    } catch (Exception e) {
                        Program.Log.Error($"API Error: {cleanCmd} - {e.Message}");
                    }
                }
            }
        }
    }
    return segments;
}

/**
 * Returns the total available system or container memory in Gigabytes.
 */
public static long GetTotalMemoryGB()
{
    // Returns the total system (or container) memory in bytes
    var stats = GC.GetGCMemoryInfo();
    long totalMemoryBytes = stats.TotalAvailableMemoryBytes;

    // If for any reason it returns 0 or less, use a safe default of 16GB
    if (totalMemoryBytes &lt;= 0) return 16;

    return totalMemoryBytes / 1024 / 1024 / 1024;
}

/**
 * Returns optimized LLama model parameters based on the detected system RAM.
 */
public ModelParams GetOptimizedParams(string modelPath)
{
    long totalRamGB = GetTotalMemoryGB();
    var p = new ModelParams(modelPath);

    // Common settings for all profiles
    p.FlashAttention = true;
    p.UseMemorymap = true;
    p.GpuLayerCount = 33; // Attempt GPU offloading; falls back to CPU automatically if no GPU is detected

    if (totalRamGB &lt;= 8)
    {
        // LOW RAM PROFILE (e.g., Raspberry Pi or older PCs)
        p.ContextSize = 4096;
        p.TypeK = GGMLType.GGML_TYPE_Q4_0;
        p.TypeV = GGMLType.GGML_TYPE_Q4_0;
        p.BatchSize = 128;
        p.UBatchSize = 128;
    }
    else if (totalRamGB &lt;= 16)
    {
        // STANDARD PROFILE (The most common configuration)
        p.ContextSize = 16384;
        p.TypeK = GGMLType.GGML_TYPE_Q4_0;
        p.TypeV = GGMLType.GGML_TYPE_Q4_0;
        p.BatchSize = 512;
        p.UBatchSize = 512;
    }
    else
    {
        // HIGH PERFORMANCE PROFILE (32GB RAM or more)
        p.ContextSize = 32768; // Can be safely increased to 65536 if needed
        p.TypeK = GGMLType.GGML_TYPE_Q8_0; // Use Q8 for higher precision KV cache
        p.TypeV = GGMLType.GGML_TYPE_Q8_0;
        p.BatchSize = 512;
        p.UBatchSize = 512;
    }

    return p;
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>lailama</Id>
      <Version>1.0.8</Version>
      <Required>true</Required>
      <Checksum>72D704836A3B445B5F72BC38EFE7BDEE</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>940</Address>
    <Name>Lailama</Name>
    <Description>Harness the power of Generative AI on your hardware. Chat, write code, or build advanced automations with the security of a local and private system.</Description>
    <Group>AI - Machine Learning</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>Program
  .AddFeature(
    "",
    ForCameraInputType,
    ObjectDetection,
    "Enable objects detection",
    "checkbox"
  ).AddFeature(
    "",
    ForCameraInputType,
    ObjectDetectionTrigger,
    "Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)",
    "text"
  ).AddOption(
    YoloModelPath,
    Data.GetFolder() + "/yolo26n.onnx",
    "1. Path of YOLO model file (.onnx)",
    "text"
  ).AddOption(
    YoloRequestsPerSecond,
    "5",
    "2. Max Requests Per Second (RPS)",
    "slider:1:30:1"
  );

Program.Run();
</ScriptSetup>
    <ScriptSource>var inputModules = Modules.WithFeature( ObjectDetection );
var requestsPerSecond = Program.Option(YoloRequestsPerSecond);

// Show toggle icon for this algorithm in the NVR / Video widget
Program
    .Emit("NVR.Vision.Algorithm", ObjectDetection)            // feature to toggle "On" in order to enable this algo
    .Emit("NVR.Vision.Algorithm.Icon", "detection_and_zone"); // material symbol icon for the toggle button

// Restart program if configuration has been changed to apply new settings.
When.ModuleParameterChanged( (module, property) =&gt; {
    if (module.Instance == Program.Module &amp;&amp; property.Name.StartsWith("ConfigureOptions."))
    {
        if (Program.IsRunning) Program.Restart();
        return true;
    }
    return true;
});

var modelPath = Program.Option(YoloModelPath)?.Value;
if (String.IsNullOrEmpty(modelPath))
{
    Program.Notify($"Configure the path of YOLO 'detect' model (.onnx file). {OptionButtons}");
    Pause(5);
    return;
}


// For details about this implemention see
// *ML.net* and *YoloSharp* documentation


try
{
    var errorOccurred = false;
    using var detectPredictor = new YoloPredictor(modelPath);
    while (Program.IsRunning)
    {
        if (inputModules.SelectedModules.Count == 0)
        {
            Pause(1);
            continue;
        }
        inputModules.Command("Camera.GetPicture").Submit((m, data) =&gt; {
            if (data == null || ((byte[])data).Length == 0)
            {
                // I/O errors might occasionally occur and can be ignored
                return;
            }
            try
            {
                var result = detectPredictor.Detect((byte[])data, new YoloConfiguration { Confidence = 0.35f });
                //Console.WriteLine($"Result: {result}");
                //Console.WriteLine($"Speed:  {result.Speed}");
                var module = Modules.InDomain(m.Domain).WithAddress(m.Address).Get();
                if (result.Count() &gt; 0)
                {
                    var jsonResults = "";
                    // Emit "Sensor.ObjectDetect.Subject.Data" event if anything
                    // mathing the configured "TriggerDetect" list
                    // was detected in the scene.
                    string[] matchList = module
                        .Parameter( ObjectDetectionTrigger )?.Value
                        .Split(',').Select(p =&gt; p.Trim())
                        .Where(x =&gt; !string.IsNullOrEmpty(x))
                        .ToArray();
                    if (matchList.Length &gt; 0)
                    {
                        var filtered = new List&lt;Compunet.YoloSharp.Data.Detection&gt;();
                        foreach (var r in result)
                        {
                            var subject = r.Name.Name;
                            if (matchList.Contains(subject))
                            {
                                filtered.Add(r);
                            }
                        }
                        if (filtered.Count &gt; 0)
                        {
                            if (module.Parameter(ObjectDetect).DecimalValue != filtered.Count)
                            {
                                module.Emit(ObjectDetect, filtered.Count);
                            }
                            else
                            {
                                // update value and timestamp only, do not emit event
                                module.Parameter(ObjectDetect).SetData(filtered.Count);
                            }
                            foreach (var r in filtered)
                            {
                                module.Emit(ObjectDetectSubject, r);
                            }
                            jsonResults = JsonConvert.SerializeObject(filtered);
                        }
                        else if (module.Parameter(ObjectDetect).DecimalValue != 0 &amp;&amp; module.Parameter(ObjectDetect).IdleTime &gt; 5)
                        {
                            module.Emit(ObjectDetect, 0);
                        }
                    }
                    else
                    {
                        jsonResults = JsonConvert.SerializeObject(result);
                    }

                    if (jsonResults != "")
                    {
                        // Emit json data for the video player (overlay data)
                        module.Emit(VideoPlayerWidgetOverlayDetect, jsonResults);
                    }
                }
            }
            catch (Exception e)
            {
                errorOccurred = true;
                //Console.WriteLine(e.Message);
            }
            if (errorOccurred)
            {
                errorOccurred = false;
                Pause(3);
            }
        });
        if (requestsPerSecond.DecimalValue &gt; 0)
        {
            Pause(1D / requestsPerSecond.DecimalValue);
        }
        else
        {
            Pause(0.2); // default is 5 RPS max
        }
    }
}
catch (Exception e)
{
    Program.Notify($"Error: {e.Message} {OptionButtons}");
    Pause(5);
    return;
}
</ScriptSource>
    <ScriptContext>const string
OptionButtons = "[program_configure,program_disable]",
YoloModelPath = "Yolo.ModelPath",
YoloRequestsPerSecond = "Yolo.RequestsPerSecond",
ForCameraInputType = "VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/",
ObjectDetection = "ML.ObjectDetection",
ObjectDetectionTrigger = $"{ObjectDetection}.TriggerDetect",
ObjectDetect = "Sensor.ObjectDetect",
ObjectDetectSubject = "Sensor.ObjectDetect.Subject",
VideoPlayerWidgetOverlayDetect = "Widget.Data.VideoPlayer.Overlay.Detect";
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>object-detection</Id>
      <Version>1.0.3</Version>
      <Required>true</Required>
      <Checksum>9BBE2416C9A0E9AC60ADCEF095CA1C86</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>910</Address>
    <Name>Object Detection</Name>
    <Description>Detect objects using a pre-trained YOLO model.
</Description>
    <Group>AI - Machine Learning</Group>
    <Features>
      <ProgramFeature>
        <FieldType>checkbox</FieldType>
        <ForDomains />
        <ForTypes>VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/</ForTypes>
        <Property>ML.ObjectDetection</Property>
        <Description>Enable objects detection</Description>
      </ProgramFeature>
      <ProgramFeature>
        <FieldType>text</FieldType>
        <ForDomains />
        <ForTypes>VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/</ForTypes>
        <Property>ML.ObjectDetection.TriggerDetect</Property>
        <Description>Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)</Description>
      </ProgramFeature>
    </Features>
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>Program
  .AddFeature(
    "",
    ForCameraInputType,
    PoseDetection,
    "Enable pose tracking",
    "checkbox"
  ).AddOption(
    "Yolo.ModelPath",
    Data.GetFolder() + "/yolo26n-pose.onnx",
    "1. Path of YOLO model file (.onnx)",
    "text"
  ).AddOption(
    YoloRequestsPerSecond,
    "5",
    "2. Max Requests Per Second (RPS)",
    "slider:1:30:1"
  );

Program.Run();
</ScriptSetup>
    <ScriptSource>var inputModules = Modules.WithFeature(PoseDetection);
var requestsPerSecond = Program.Option(YoloRequestsPerSecond);

// Show toggle icon for this algorithm in the NVR / Video widget
Program
    .Emit("NVR.Vision.Algorithm", PoseDetection)     // feature to toggle "On" in order to enable this algo
    .Emit("NVR.Vision.Algorithm.Icon", "sports_gymnastics");          // material symbol icon for the toggle button

// Restart program if configuration has been changed to apply new settings.
When.ModuleParameterChanged( (module, property) =&gt; {
  if (module.Instance == Program.Module &amp;&amp; property.Name.StartsWith("ConfigureOptions."))
  {
    if (Program.IsRunning) Program.Restart();
    return true;
  }
  return true;
});

var yoloModelPath = Program.Option("Yolo.ModelPath")?.Value;
if (String.IsNullOrEmpty(yoloModelPath))
{
    Program.Notify($"Configure the path of YOLO 'pose' model (.onnx file). {OptionButtons}");
    Pause(5);
    return;
}


// For details about this implemention see
// *ML.net* and *YoloSharp* documentation


try
{
    var errorOccurred = false;
    using var detectPredictor = new YoloPredictor(yoloModelPath);
    while (Program.IsRunning)
    {
        if (inputModules.SelectedModules.Count == 0)
        {
            Pause(1);
            continue;
        }
        inputModules.Command("Camera.GetPicture").Submit((m, data) =&gt; {
            if (data == null || ((byte[])data).Length == 0)
            {
                // I/O errors might occasionally occur and can be ignored
                return;
            }
            try
            {
                var module = Modules.InDomain(m.Domain).WithAddress(m.Address).Get();
                var result = detectPredictor.Pose((byte[])data);
                //Console.WriteLine($"Result: {result}");
                //Console.WriteLine($"Speed:  {result.Speed}");
                if (result.Count() &gt; 0)
                {
                    var jsonResults = JsonConvert.SerializeObject(result);
                    // Emit json data for the video player (overlay data)
                    module.Emit(VideoPlayerWidgetOverlayPose, jsonResults);

                    if (module.Parameter(ObjectDetect).DecimalValue != result.Count)
                    {
                        module.Emit(ObjectDetect, result.Count);
                    }
                    else
                    {
                        // update value and timestamp only, do not emit event
                        module.Parameter(ObjectDetect).SetData(result.Count);
                    }

                    //foreach (var r in result)
                    //{
                    //    module.Emit(ObjectDetectSubject, r);
                    //}
                }
                else if (module.Parameter(ObjectDetect).DecimalValue != 0 &amp;&amp; module.Parameter(ObjectDetect).IdleTime &gt; 5)
                {
                    module.Emit(ObjectDetect, 0);
                }
            }
            catch (Exception e)
            {
                errorOccurred = true;
                Program.Notify(e.Message);
            }
        });
        if (errorOccurred)
        {
            errorOccurred = false;
            Pause(3);
        }
        else
        {
            if (requestsPerSecond.DecimalValue &gt; 0)
            {
                Pause(1D / requestsPerSecond.DecimalValue);
            }
            else
            {
                Pause(0.2); // default is 5 RPS max
            }
        }
    }
}
catch (Exception e)
{
    Program.Notify($"Error: {e.Message} {OptionButtons}");
    Pause(5);
    return;
}
</ScriptSource>
    <ScriptContext>const string
OptionButtons = "[program_configure,program_disable]",
YoloModelPath = "Yolo.ModelPath",
YoloRequestsPerSecond = "Yolo.RequestsPerSecond",
ForCameraInputType = "VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/",
PoseDetection = "ML.PoseEstimation",
ObjectDetect = "Sensor.ObjectDetect",
ObjectDetectSubject = "Sensor.ObjectDetect.Subject",
VideoPlayerWidgetOverlayPose = "Widget.Data.VideoPlayer.Overlay.Pose";
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.4</PackageVersion>
      <Id>pose-estimation</Id>
      <Version>1.0.3</Version>
      <Required>true</Required>
      <Checksum>B6C921277F8E9E598F3050ED3D92A7AA</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>911</Address>
    <Name>Pose estimation</Name>
    <Description>Keypoint detection via pose estimation, using custom ONNX models or the default pre-trained YOLO model (human pose-specific).</Description>
    <Group>AI - Machine Learning</Group>
    <Features>
      <ProgramFeature>
        <FieldType>checkbox</FieldType>
        <ForDomains />
        <ForTypes>VideoInput,Sensor:Widget.DisplayModule=/homegenie/generic/(videoinput|camerainput)/</ForTypes>
        <Property>ML.PoseEstimation</Property>
        <Description>Enable pose tracking</Description>
      </ProgramFeature>
    </Features>
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
</ArrayOfProgramBlock>