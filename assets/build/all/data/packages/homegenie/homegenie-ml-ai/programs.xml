<?xml version="1.0" encoding="utf-8"?>
<ArrayOfProgramBlock xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xmlns:xsd="http://www.w3.org/2001/XMLSchema">
  <ProgramBlock>
    <ScriptSetup>// This method is used to register this program as a "AI intent handler"
Program.Implements(
    "@AI:IntentHandler",
    API_URL
).AddOption("ApiKey", "?", "Google Gemini AI API key", "text")
 .AddOption("ModelId", DEFAULT_MODEL_ID, "Model ID", "text")
 .Run();
</ScriptSetup>
    <ScriptSource>#using System.Text.RegularExpressions

// Define constants for API paths
const string API_PROMPT_SUBMIT = "Prompt.Submit";
const string API_HISTORY_GET = "History.Get";
const string API_HISTORY_CLEAR = "History.Clear";

// Load chat history at startup
ChatSession.LoadHistory(Data.GetFolder());

// ===================================================================
// API HANDLER: Submit a new intent (user command)
// ===================================================================
Api.Handle($"{API_URL}/{API_PROMPT_SUBMIT}", (userCommand) =&gt; 
{
    // 1. Validate configuration
    string geminiApiKey = Program.Option("ApiKey").Value.Trim();
    string geminiModelId = Program.Option("ModelId").Value.Trim();
    if (geminiApiKey.Length != 39 || !geminiApiKey.StartsWith("AIzaSy") || string.IsNullOrEmpty(geminiModelId))
    {
        string error = "Gemini Intent Handler is not properly configured.";
        Program.Notify($"ERROR: {error} Please configure the program options.");
        return new ResponseText($"**Error:** {error} Go to the program's settings page to configure it.");
    }

    // 2. Build the current device list
    string allowedTypes = "Dimmer,Light,Color,Switch,Shutter,DoorLock,Thermostat";
    var devicesInGroups = Modules.Groups
        .SelectMany(
            groupName =&gt; Modules.InGroup(groupName).OfDeviceType(allowedTypes).SelectedModules,
            (groupName, module) =&gt; new { name = module.Name, group = groupName, address = module.Address, domain = module.Domain, type = module.DeviceType }
        ).ToList();
    var uniqueIdsInGroups = new HashSet&lt;(string, string)&gt;(devicesInGroups.Select(d =&gt; (d.domain, d.address)));
    var devicesWithoutGroup = Modules.OfDeviceType(allowedTypes).SelectedModules
        .Where(module =&gt; !uniqueIdsInGroups.Contains((module.Domain, module.Address)))
        .Select(module =&gt; new { name = module.Name, group = "", address = module.Address, domain = module.Domain, type = module.DeviceType });
    devicesInGroups.AddRange(devicesWithoutGroup);
    // Add Alarm System program module
    devicesInGroups.Add(new { name = "Security Alarm System", group = "Dashboard", address = "90", domain = "HomeAutomation.HomeGenie.Automation", type = ModuleTypes.Sensor });
    // Add all scripts/macros
    var scriptMacros = Modules.OfDeviceType("Program").WithParameter("Widget.DisplayModule").SelectedModules
        .Where(module =&gt; module.Address != "6" &amp;&amp; module.Address != "7" &amp;&amp; module.Properties.Find((p) =&gt; p.Name == "Widget.DisplayModule").Value == "homegenie/generic/program")
        .Select(module =&gt; new { name = module.Name, group = "", address = module.Address, domain = module.Domain, type = module.DeviceType });
    devicesInGroups.AddRange(scriptMacros);

    //string devicesList = JsonConvert.SerializeObject(devicesInGroups, Formatting.Indented);
    string devicesList = "";
    foreach (var m in devicesInGroups) {
      devicesList += $"- {m.name} | {m.type} | {m.group} | {m.domain}/{m.address}\n";
    }

    // 3. Prepare the user message with context
    var userMessage = new ChatMessage
    {
        Role = "user",
        Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = userCommand as string } }
    };
    var payloadContents = new List&lt;ChatMessage&gt;(ChatSession.History);
    payloadContents.Add(userMessage);
    
    var payload = new
    {
        contents = payloadContents,
        system_instruction = new { parts = new[] { new { text = GEMINI_SYSTEM_PROMPT.Replace("%%AVAILABLE_DEVICES%%", devicesList) } } },
        generationConfig = new { temperature = 0.3 }
    };

    try
    {
        // 4. Send the request to Gemini API
        string payloadJson = JsonConvert.SerializeObject(payload);
        string geminiRequestUrl = $"https://generativelanguage.googleapis.com/v1beta/models/{geminiModelId}:generateContent?key={geminiApiKey}";
        var data = Net.WebService(geminiRequestUrl).WithTimeout(120).AddHeader("Content-Type", "application/json").Post(payloadJson).GetData();

        // 5. Process the response
        string rawGeminiResponseJson = data?.candidates?[0]?.content?.parts?[0]?.text;

        if (string.IsNullOrEmpty(rawGeminiResponseJson))
        {
            string error = "Received an empty or invalid response from the AI.";
            Program.Notify($"ERROR: {error}");
            return new ResponseText($"**Error:** {error}");
        }

        string jsonResponse = "";
        string thoughtsChain = rawGeminiResponseJson;

        string patternJsonBlock = @"```json\s*({[\s\S]*?})\s*```";
        var matchJsonBlock = Regex.Match(rawGeminiResponseJson, patternJsonBlock, RegexOptions.Singleline);

        if (matchJsonBlock.Success)
        {
            jsonResponse = matchJsonBlock.Groups[1].Value;
            thoughtsChain = rawGeminiResponseJson.Substring(0, matchJsonBlock.Index).Trim();
        }
        else
        {
            int lastBraceIndex = rawGeminiResponseJson.LastIndexOf('}');
            if (lastBraceIndex != -1)
            {
                int firstBraceIndex = rawGeminiResponseJson.LastIndexOf('{', lastBraceIndex);
                if (firstBraceIndex != -1)
                {
                    string potentialJson = rawGeminiResponseJson.Substring(firstBraceIndex);
                    try
                    {
                        JObject.Parse(potentialJson); // Validate JSON
                        jsonResponse = potentialJson;
                        thoughtsChain = rawGeminiResponseJson.Substring(0, firstBraceIndex).Trim();
                    }
                    catch (JsonReaderException) { /* Invalid block, ignored */ }
                }
            }
        }

        if (string.IsNullOrEmpty(jsonResponse))
        {
            var fallbackResponse = new {
                answer = rawGeminiResponseJson,
                commands = new string[] {}
            };
            jsonResponse = JsonConvert.SerializeObject(fallbackResponse);
            thoughtsChain = "N/A (No valid JSON block found in the response)";
            Program.Log.Warn("Could not extract a valid JSON block from AI response.");
        }

        if (!string.IsNullOrEmpty(thoughtsChain))
        {
            Program.Log.Info($"AI Thought Process: {thoughtsChain}");
        }

        string friendlyAnswer = rawGeminiResponseJson;
        try 
        {
            dynamic hgCommand = JsonConvert.DeserializeObject&lt;dynamic&gt;(jsonResponse);
            friendlyAnswer = hgCommand.answer.ToString();
            JArray commands = hgCommand.commands;

            if (commands != null)
            {
                foreach (var commandToken in commands)
                {
                    string command = commandToken.ToString();
                    if (!string.IsNullOrEmpty(command) &amp;&amp; command.ToUpper() != "NONE")
                    {
                        Program.Log.Info($"Executing command from Gemini: {command}");
                        Api.Call(command);
                    }
                }
            }
        } 
        catch (Exception parseEx) 
        {
            Program.Log.Warn($"Could not parse Gemini response as JSON. Treating as plain text. Error: {parseEx.Message}");
        }

        // 6. Update and save history
        ChatSession.History.Add(userMessage);
        var modelResponse = new ChatMessage
        {
            Role = "model",
            Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = jsonResponse } }
        };
        ChatSession.History.Add(modelResponse);
        ChatSession.SaveHistory(Data.GetFolder());

        // 7. Return the friendly answer to the UI
        return new ResponseText(friendlyAnswer);
    }
    catch (Exception ex)
    {
        Program.Notify($"Error while calling Gemini API: {ex.Message}");
        return new ResponseText($"**Error:** An exception occurred while contacting the AI service. Please check the logs.\n\n`{ex.Message}`");
    }
});


// ===================================================================
// API HANDLER: Get the current chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_GET}", (userCommand) =&gt; 
{
    return ChatSession.History;
});


// ===================================================================
// API HANDLER: Clear the chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_CLEAR}", (userCommand) =&gt; 
{
    try
    {
        ChatSession.History.Clear();
        ChatSession.SaveHistory(Data.GetFolder());
        Program.Log.Info("Gemini Intent Handler chat history has been cleared.");
        return new ResponseStatus(Status.Ok);
    }
    catch (Exception ex)
    {
        Program.Notify($"Error clearing chat history: {ex.Message}");
        return new ResponseStatus(Status.Error, "Could not clear history.");
    }
});


// Run the program in the background
Program.GoBackground();
</ScriptSource>
    <ScriptContext>const string API_URL = "AI.IntentHandlers/Gemini";
const string DEFAULT_MODEL_ID = "gemini-flash-latest";

const string GEMINI_SYSTEM_PROMPT = """

You are a home automation assistant called HomeGenie.


**RULES**

1.  **JSON ONLY:** Your entire response MUST be ONLY a valid JSON object. No markdown. No chain thoughts.
2.  **USER'S LANGUAGE:** The "answer" field MUST be in the same language as the user's command.
3.  **COMMAND ARRAY:** The "commands" field must be an array of strings. The reply might not contains a command in which case the `commands` array must be `["NONE"]`.


**AVAILABLE DEVICES**
%%AVAILABLE_DEVICES%%


**COMMANDS LIST**
-   `Control.On`, `Control.Off`
-   `Control.Level/{value}` (value: 0-100)
-   `Control.ColorHsb/{h},{s},{v}` (h,s,v: 0.0-1.0)
-   `Thermostat.ModeSet/{mode}` (mode: "Off", "Heat", "HeatEconomy", "Cool", "CoolEconomy")
-   `Thermostat.SetPointSet/{type}/{value}` (type: "Heating", "HeatingEconomy", "Cooling", "CoolingEconomy")
-   `Control.ArmAway`
-   `Control.ArmHome`
-   `Control.Disarm`

**EXCEPTIONS**
For type `Program` commands are in the form:
-   `/api/HomeAutomation.HomeGenie/Automation/Programs.Run/&lt;numeric_address&gt;
-   `/api/HomeAutomation.HomeGenie/Automation/Programs.Break/&lt;numeric_address&gt;


**EXAMPLE**

&lt;|im_start|&gt;user
Spegni tutte le luci dello studio e poi accendi la caldaia e anche i riscaldamenti.&lt;|im_end|&gt;

&lt;|im_start|&gt;assistant
{
  "answer": "Fatto! Ho spento la luce della scrivania e Ambient nello Studio e ho acceso la caldaia.",
  "commands": [
    "/api/HomeAutomation.PhilipsHue/3/Control.Off",
    "/api/HomeAutomation.ZigBee/5/Control.Off",
    "/api/HomeAutomation.HomeGenie/S2/Control.On",
    "/api/HomeAutomation.SmartThermostat/1/Thermostat.ModeSet/Heat"
  ]
}
&lt;|im_end|&gt;

""";


// POJO

public class ChatMessage
{
    [JsonProperty("role")]
    public string Role { get; set; }

    [JsonProperty("parts")]
    public List&lt;MessagePart&gt; Parts { get; set; }
}

public class MessagePart
{
    [JsonProperty("text")]
    public string Text { get; set; }
}

static class ChatSession
{
    public static List&lt;ChatMessage&gt; History { get; set; } = new List&lt;ChatMessage&gt;();
    private static readonly string FileName = "gemini_chathistory.json";

    public static void SaveHistory(string path)
    {
        string filePath = Path.Combine(path, FileName);
        try
        {
            string json = JsonConvert.SerializeObject(History, Formatting.Indented);
            File.WriteAllText(filePath, json);
        }
        catch (Exception ex)
        {
            //Program.Log.Error($"Failed to save chat history: {ex.Message}");
        }
    }

    public static void LoadHistory(string path)
    {
        string filePath = Path.Combine(path, FileName);
        try
        {
            if (File.Exists(filePath))
            {
                string json = File.ReadAllText(filePath);
                var loadedHistory = JsonConvert.DeserializeObject&lt;List&lt;ChatMessage&gt;&gt;(json);
                if (loadedHistory != null)
                {
                    History = loadedHistory;
                }
            }
        }
        catch (Exception ex)
        {
            History = new List&lt;ChatMessage&gt;();
        }
    }
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.3</PackageVersion>
      <Id>gemini-intent-handler</Id>
      <Version>1.0.5</Version>
      <Required>true</Required>
      <Checksum>E6A95A95F71C03A9C1326BB29387D340</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>930</Address>
    <Name>Gemini Intent Handler</Name>
    <Description>Enables natural language control for your smart system using Google's Gemini AI.
To activate this feature, enter a valid API key.
Follow the instructions at https://aistudio.google.com/app/apikey to get a free API key.</Description>
    <Group>AI - Machine Learning</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>true</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>// This method is used to register this program as a "AI intent handler"
Program.Implements(
    "@AI:WidgetGenie",
    API_URL
).AddOption("ApiKey", "?", "Google Gemini AI API key", "text")
 .AddOption("ModelId", DEFAULT_MODEL_ID, "Model ID", "text")
 .Run();
</ScriptSetup>
    <ScriptSource>// Define constants for API paths to avoid magic strings
const string API_PROCESS = "Prompt.Submit";
const string API_HISTORY_GET = "History.Get";
const string API_HISTORY_CLEAR = "History.Clear";

// Load the chat history from disk at startup
ChatSession.LoadHistory(Data.GetFolder());

// ===================================================================
// API HANDLER: Process a new user message
// ===================================================================
Api.Handle($"{API_URL}/{API_PROCESS}", (userCommand) =&gt; 
{
    // 1. Validate configuration
    string geminiApiKey = Program.Option("ApiKey").Value.Trim();
    string geminiModelId = Program.Option("ModelId").Value.Trim();
    if (geminiApiKey.Length != 39 || !geminiApiKey.StartsWith("AIzaSy") || string.IsNullOrEmpty(geminiModelId))
    {
        string error = "Gemini AI Genie is not properly configured.";
        Program.Notify($"ERROR: {error} Please configure the program options.");
        return new ResponseText($"**Error:** {error} Go to the program's settings page to configure it.");
    }

    // 2. Prepare the user message
    var userMessage = new ChatMessage
    {
        Role = "user",
        Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = userCommand as string } }
    };

    // 3. Build the payload with the full history
    var payloadContents = new List&lt;ChatMessage&gt;(ChatSession.History);
    payloadContents.Add(userMessage);
    
    var payload = new
    {
        contents = payloadContents,
        system_instruction = new
        {
            parts = new[]
            {
                new { text = GEMINI_SYSTEM_PROMPT }
            }
        },
        generationConfig = new
        {
            temperature = 0.3
        }
    };

    try
    {
        // 4. Send the request to Gemini API
        string payloadJson = JsonConvert.SerializeObject(payload);
        string geminiRequestUrl = $"https://generativelanguage.googleapis.com/v1beta/models/{geminiModelId}:generateContent?key={geminiApiKey}";
        
        var data = Net
            .WebService(geminiRequestUrl)
            .WithTimeout(120)
            .AddHeader("Content-Type", "application/json")
            .Post(payloadJson)
            .GetData();

        // 5. Process the response
        // Using `?.` (null-conditional operator) for safe navigation
        string geminiResponse = data?.candidates?[0]?.content?.parts?[0]?.text;

        if (!string.IsNullOrEmpty(geminiResponse))
        {
            // Add both user message and model response to history
            ChatSession.History.Add(userMessage);
            var modelResponse = new ChatMessage
            {
                Role = "model",
                Parts = new List&lt;MessagePart&gt; { new MessagePart { Text = geminiResponse } }
            };
            ChatSession.History.Add(modelResponse);
            ChatSession.SaveHistory(Data.GetFolder());

            return new ResponseText(geminiResponse);
        }
        else
        {
            // Handle cases where Gemini returns an empty or malformed response
            string error = "Received an empty or invalid response from the AI.";
            Program.Notify($"ERROR: {error}");
            return new ResponseText($"**Error:** {error}");
        }
    }
    catch (Exception ex)
    {
        Program.Notify($"Error while calling Gemini API: {ex.Message}");
        return new ResponseText($"**Error:** An exception occurred while contacting the AI service. Please check the logs.\n\n`{ex.Message}`");
    }
});

// ===================================================================
// API HANDLER: Get the current chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_GET}", (userCommand) =&gt; 
{
    return ChatSession.History;
});

// ===================================================================
// API HANDLER: Clear the chat history
// ===================================================================
Api.Handle($"{API_URL}/{API_HISTORY_CLEAR}", (userCommand) =&gt; 
{
    try
    {
        // Clear the in-memory history list
        ChatSession.History.Clear();
        // Save the empty history to disk to persist the change
        ChatSession.SaveHistory(Data.GetFolder());
        
        Program.Log.Info("AI Genie chat history has been cleared.");
        return new ResponseStatus(Status.Ok);
    }
    catch (Exception ex)
    {
        Program.Notify($"Error clearing chat history: {ex.Message}");
        return new ResponseStatus(Status.Error, "Could not clear history.");
    }
});

// Run the program in the background
Program.GoBackground();</ScriptSource>
    <ScriptContext>const string API_URL = "AI.WidgetGenie/Gemini";
const string DEFAULT_MODEL_ID = "gemini-flash-latest";

const string GEMINI_SYSTEM_PROMPT = """

You are an expert AI assistant for the HomeGenie Server platform.
Your ONLY task is to help users create custom widgets based on the zuix.js framework and HomeGenie Widget API.

A widget consists of three code blocks: HTML (View), CSS (Style), and JavaScript (Controller).

## Core Concepts
- **Controller:** A class extending `ControllerInstance` with life-cycle callbacks (`onCreate`, etc.).
- **View:** Standard HTML. Elements are linked to the controller using `#field_name`.
- **Data Binding:** The `this.model()` object is a reactive proxy. Changes to `this.model().field_name` automatically update the corresponding `#field_name` element in the view.
- **Events:** Use `(click)="myFunction()"` in HTML. The function must be exposed in `onInit` via `this.declare({ myFunction: this.myFunction.bind(this) })`.

---
**API REFERENCE &amp; MANDATORY RULES**
---

**1. ZUIX.JS &amp; VIEW LOGIC -- CRITICAL RULES**
   - **FATAL: NO `{{ }}` INTERPOLATION.** Never use `{{ expression }}` in the HTML. Dynamic updates MUST be handled in the controller by updating the model.
   - **DATA BINDING IS KING:** All dynamic UI updates MUST be driven by modifying the `this.model()` object. NEVER manipulate the DOM directly with `this.field('...').html()`.
   - **PROVIDE DEFAULT VALUES IN HTML:** The HTML view MUST always contain sensible default content for preview purposes. NEVER generate empty tags for fields.
     - **WRONG:** `&lt;span #status&gt;&lt;/span&gt;`
     - **CORRECT:** `&lt;span #status&gt;Disarmed&lt;/span&gt;`

**2. HOMEGENIE API -- CRITICAL RULES**
   - **Module Fields (`field.key`):**
     - Use `this.boundModule.field('Key.Name')` to get a specific field. DO NOT USE `this.boundModule.fields.find(...)`.
     - **Available Keys:** `Status.Level`, `Status.ColorHsb`, `Sensor.Temperature`, `Conditions.IconType`, `HomeGenie.SecurityArmed` (values: "Away", "Home", "Disarmed").
     - **STATE LOGIC:** NEVER use `Status.Power`. The ON/OFF state is ALWAYS derived from `Status.Level` (a level &gt; 0 means ON).
   - **Module Commands (`control()`):**
     - **Available Commands:** `Control.On`, `Control.Off`, `Control.Level/&lt;level&gt;`, `Control.ColorHsb/&lt;h&gt;,&lt;s&gt;,&lt;b&gt;`, `Control.ArmAway`, `Control.ArmHome`, `Control.Disarm`.
     - **COMMAND LOGIC:** For ON/OFF, prefer `Control.On` and `Control.Off`.

**3. DATA FORMATTING -- CRITICAL RULES**
   - **DECIMAL SEPARATORS:** `field.value` is a string. Before `parseFloat()`, YOU MUST use `.replace(',', '.')`.

**4. ICONS -- CRITICAL RULES**
   - **SVG ICONS:** Use an `&lt;img&gt;` tag. The model contains the path, and the view binds it to the `src` attribute. Example: `&lt;img #icon src="path/to/default.svg"&gt;`.

**OUTPUT FORMATTING -- MANDATORY**
1.  **For widget requests:** Respond ONLY with three markdown code blocks (html, css, javascript).
2.  **For other requests:** Respond with conversational text.
3.  **LANGUAGE RULE:** Generated code and code comments MUST always be in English. Conversational text (e.g., for explanations or questions) MUST be in the same language as the user's last prompt.

---
**GOLDEN EXAMPLE: Switch Widget**
---

*If a user asks "create a widget for a switch", the output MUST follow this exact structure.*
When generating a complete widget, you must also provide a brief, friendly explanation of the generated code.

Your response structure MUST be as follows:
1.  A short, conversational introduction in the user's language.
2.  A clear explanation of the purpose and functionality of the widget.
3.  The three code blocks (HTML, CSS, JavaScript), clearly labeled.
4.  A concluding sentence guiding the user to copy and paste each block into the corresponding editor tab (View, Style, Controller).

**EXAMPLE OF THE DESIRED RESPONSE STRUCTURE:**

Of course! Here is a basic widget for a switch. It displays the switch's name and status, with a button to toggle it on and off. The button color will change to reflect the current state.

Simply copy and paste each of the following code blocks into the corresponding editor tabs: View (HTML), Style (CSS), and Controller (JavaScript).

Finally, remember to bind a module (the device) to this widget by clicking the settings button on the editor toolbar.

```html
&lt;div class="container"&gt;
  &lt;strong #name&gt;Unknown Switch&lt;/strong&gt;
  &lt;button (click)="toggleSwitch()"&gt;
    &lt;span class="material-symbols-outlined" #icon&gt;toggle_off&lt;/span&gt;
  &lt;/button&gt;
  &lt;span #status&gt;Off&lt;/span&gt;
&lt;/div&gt;
&lt;link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Material+Symbols+Outlined"&gt;
```
```css
:host { display: block; }
.container { padding: 16px; display: flex; align-items: center; gap: 12px; }
button { background: none; border: none; cursor: pointer; }
.material-symbols-outlined { font-size: 48px; }
```
```javascript
class SwitchWidget extends ControllerInstance {

  isOn = false;

  onCreate() {
    // Expose the toggleSwitch method to the HTML View
    this.declare({
      toggleSwitch: this.toggleSwitch
    });

    if (!this.boundModule) {
      return;
    }

    // Subscribe to the external module's state
    const levelField = this.boundModule.field('Status.Level');
    if (levelField) {
      this.subscribe(levelField, (field) =&gt; {
        // When the external state changes, update the internal state
        this.updateState(field.value);
      });
    }
    
    // Perform an initial sync when the widget is created
    this.updateModel();
  }
  
  updateModel() {
      this.model().name = this.boundModule.name;
      const levelField = this.boundModule.field('Status.Level');
      if (levelField) {
        this.updateState(levelField.value);
      }
  }

  updateState(levelValue) {
    const level = parseFloat(levelValue.replace(',', '.'));
    this.isOn = level &gt; 0;
    
    // Update the reactive model. The view will update automatically.
    this.model().status = this.isOn ? 'On' : 'Off';
    this.model().icon = this.isOn ? 'toggle_on' : 'toggle_off';
  }

  toggleSwitch() {
    if (!this.boundModule) return;
    
    const command = this.isOn ? 'Control.Off' : 'Control.On';
    this.boundModule.control(command);
    /*
    or if you want to add a callback when the command is completed
    this.boundModule.control(command).subscribe((res) =&gt; {
      // command completed
    });
    */
  }
}
```

Now, begin.
""";


// POJO

public class ChatMessage
{
    [JsonProperty("role")]
    public string Role { get; set; }

    [JsonProperty("parts")]
    public List&lt;MessagePart&gt; Parts { get; set; }
}

public class MessagePart
{
    [JsonProperty("text")]
    public string Text { get; set; }
}

static class ChatSession
{
    public static List&lt;ChatMessage&gt; History { get; set; } = new List&lt;ChatMessage&gt;();
    private static readonly string FileName = "gemini_chathistory.json";

    public static void SaveHistory(string path)
    {
        string filePath = Path.Combine(path, FileName);
        try
        {
            string json = JsonConvert.SerializeObject(History, Formatting.Indented);
            File.WriteAllText(filePath, json);
        }
        catch (Exception ex)
        {
            //Program.Log.Error($"Failed to save chat history: {ex.Message}");
        }
    }

    public static void LoadHistory(string path)
    {
        string filePath = Path.Combine(path, FileName);
        try
        {
            if (File.Exists(filePath))
            {
                string json = File.ReadAllText(filePath);
                var loadedHistory = JsonConvert.DeserializeObject&lt;List&lt;ChatMessage&gt;&gt;(json);
                if (loadedHistory != null)
                {
                    History = loadedHistory;
                }
            }
        }
        catch (Exception ex)
        {
            History = new List&lt;ChatMessage&gt;();
        }
    }
}
</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.3</PackageVersion>
      <Id>gemini-widget-genie</Id>
      <Version>1.0.2</Version>
      <Required>true</Required>
      <Checksum>BEC152E192F00605C8705023C2CBB6B6</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>931</Address>
    <Name>Gemini Widget Genie</Name>
    <Description>Your personal AI genie for widget creation. Powered by Gemini, it helps you write and debug HTML, CSS, and JavaScript code directly within the HomeGenie editor.
To activate this feature, enter a valid API key.
Follow the instructions at https://aistudio.google.com/app/apikey to get a free API key.</Description>
    <Group>AI - Machine Learning</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>Program
  .AddFeature(
    "",
    ForCameraInputType,
    InstanceSegmentation,
    "Enable instance segmentation",
    "checkbox"
  ).AddFeature(
    "",
    ForCameraInputType,
    InstanceSegmentationTrigger,
    "Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)",
    "text"
  ).AddOption(
    "Yolo.ModelPath",
    Data.GetFolder() + "/yolo11n-seg.onnx",
    "Path of YOLO model file (.onnx)",
    "text"
  );

Program.Run();
</ScriptSetup>
    <ScriptSource>var inputModules = Modules.WithFeature(InstanceSegmentation);

// Restart program if configuration has been changed to apply new settings.
When.ModuleParameterChanged( (module, property) =&gt; {
    if (module.Instance == Program.Module &amp;&amp; property.Name.StartsWith("ConfigureOptions."))
    {
        if (Program.IsRunning) Program.Restart();
        return true;
    }
    return true;
});

var yoloModelPath = Program.Option("Yolo.ModelPath")?.Value;
if (String.IsNullOrEmpty(yoloModelPath))
{
    Program.Notify($"Configure the path of YOLO 'segment' model (.onnx file). {OptionButtons}");
    Pause(5);
    return;
}


// For details about this implemention see
// *ML.net* and *YoloSharp* documentation


try
{
    var errorOccurred = false;
    using var detectPredictor = new YoloPredictor(yoloModelPath);
    while (Program.IsRunning)
    {
        if (inputModules.SelectedModules.Count == 0)
        {
            Pause(1);
            continue;
        }
        inputModules.Command("Camera.GetPicture").Submit((m, data) =&gt; {
            try
            {
                var result = detectPredictor.Segment((byte[])data);
                //Console.WriteLine($"Result: {result}");
                //Console.WriteLine($"Speed:  {result.Speed}");
                var module = Modules.InDomain(m.Domain).WithAddress(m.Address).Get();
                if (result.Count() &gt; 0)
                {
                    var output = new List&lt;SegmentResult&gt;();
                    // Emit "Sensor.ObjectDetect.Subject.Data" event if anything
                    // mathing the configured "TriggerDetect" list
                    // was detected in the scene.
                    string[] matchList = module
                        .Parameter( InstanceSegmentationTrigger )?.Value
                        .Split(',').Select(p =&gt; p.Trim())
                        .Where(x =&gt; !string.IsNullOrEmpty(x))
                        .ToArray();
                    if (matchList.Length &gt; 0)
                    {
                        var filtered = new List&lt;Compunet.YoloSharp.Data.Segmentation&gt;();
                        foreach (var r in result)
                        {
                            var subject = r.Name.Name;
                            if (matchList.Contains(subject))
                            {
                                filtered.Add(r);
                            }
                        }
                        if (filtered.Count &gt; 0)
                        {
                            if (module.Parameter(ObjectDetect).DecimalValue != filtered.Count) 
                            {
                                module.Emit(ObjectDetect, filtered.Count);
                            }
                            else
                            {
                                // update value and timestamp only, do not emit event
                                module.Parameter(ObjectDetect).SetData(filtered.Count);   
                            }
                            foreach (var r in filtered)
                            {
                                output.Add(new SegmentResult(){
                                    Result = r,
                                    Mask = ExtractAndApproximateContour(r.Mask, 100)
                                });
                                module.Emit(ObjectDetectSubject, r);
                            }
                        }
                        else if (module.Parameter(ObjectDetect).DecimalValue != 0 &amp;&amp; module.Parameter(ObjectDetect).IdleTime &gt; 5) 
                        {
                            module.Emit(ObjectDetect, 0);
                        }
                    }
                    else
                    {
                        foreach (var r in result)
                        {
                            output.Add(new SegmentResult(){
                                Result = r,
                                Mask = ExtractAndApproximateContour(r.Mask, 100)
                            });
                        }
                    }

                    if (output.Count &gt; 0)
                    {
                        // Emit json data for the video player (overlay data)
                        var jsonResults = JsonConvert.SerializeObject(output);
                        module.Emit(VideoPlayerWidgetOverlaySegment, jsonResults);
                    }

                }
            }
            catch (Exception e)
            {
                errorOccurred = true;
                Console.WriteLine(e.Message);
            }
        });
        if (errorOccurred)
        {
            errorOccurred = false;
            Pause(5);
        }
    }
}
catch (Exception e)
{
    Program.Notify($"Error: {e.Message} {OptionButtons}");
    Pause(5);
    return;
}
</ScriptSource>
    <ScriptContext>#using Compunet.YoloSharp.Memory

const string
OptionButtons = "[program_configure,program_disable]",
YoloModelPath = "Yolo.ModelPath",
ForCameraInputType = "Sensor:Widget.DisplayModule=homegenie/generic/camerainput",
InstanceSegmentation = "ML.InstanceSegmentation",
InstanceSegmentationTrigger = $"{InstanceSegmentation}.TriggerDetect",
ObjectDetect = "Sensor.ObjectDetect",
ObjectDetectSubject = "Sensor.ObjectDetect.Subject",
VideoPlayerWidgetOverlaySegment = "Widget.Data.VideoPlayer.Overlay.Segment";

public class SegmentResult
{
    public Compunet.YoloSharp.Data.Segmentation Result;
    public List&lt;MaskPoint&gt; Mask;
}

public class MaskPoint
{
    public float X;
    public float Y;
    public MaskPoint(float x, float y)
    {
        X = x;
        Y = y;
    }
}

public static List&lt;MaskPoint&gt; ExtractAndApproximateContour(BitmapBuffer r, int maxPoints = 25)
{
    var contour = ExtractContourPointsInternal(r);  // Get the initial contour

    if (contour.Count &lt;= maxPoints)
    {
        return contour; // No need to approximate if already within the limit
    }

    // Implement a simple approximation by taking equally spaced points
    var approximatedContour = new List&lt;MaskPoint&gt;();
    double interval = (double)contour.Count / maxPoints;
    for (int i = 0; i &lt; maxPoints - 1; i++)
    {
        int index = (int)Math.Round(i * interval); //Round for accuracy
        approximatedContour.Add(contour[index]); // Add existing point at the index
    }

    //Ensure the loop has the final item; If the last isn't added, add it
    if(approximatedContour.Last() != contour.Last()) {
            approximatedContour.Add(contour.Last());
    }

    return approximatedContour;
}


private static List&lt;MaskPoint&gt; ExtractContourPointsInternal(BitmapBuffer r)
{
    int height = r.Height;
    int width = r.Width;

    // Helper function to check if a pixel is a border pixel
    bool IsBorderPixel(BitmapBuffer mask, int x, int y)
    {
        if (x == 0 || x == width - 1 || y == 0 || y == height - 1)
        {
            return true;
        }

        return (mask[y, x - 1] &gt; 0.9 != mask[y, x] &gt; 0.9 ||
                mask[y, x + 1] &gt; 0.9 != mask[y, x] &gt; 0.9 ||
                mask[y - 1, x] &gt; 0.9 != mask[y, x] &gt; 0.9 ||
                mask[y + 1, x] &gt; 0.9 != mask[y, x] &gt; 0.9);
    }

    // Find all contour pixels
    var contourPixels = new List&lt;MaskPoint&gt;();
    for (int y = 0; y &lt; height; y++)
    {
        for (int x = 0; x &lt; width; x++)
        {
            if (r[y, x] &gt; 0.9 &amp;&amp; IsBorderPixel(r, x, y))
            {
                contourPixels.Add(new MaskPoint(x, y));
            }
        }
    }

    if (contourPixels.Count == 0)
    {
        return new List&lt;MaskPoint&gt;(); // Return empty list if no shape found
    }

    // Order the contour pixels (nearest neighbor approach)
    var contour = new List&lt;MaskPoint&gt;();
    var startPixel = contourPixels[0];
    contour.Add(startPixel);
    contourPixels.Remove(startPixel);

    var currentPixel = startPixel;

    while (contourPixels.Count &gt; 0)
    {
        var nearestNeighbor = contourPixels
            .OrderBy(p =&gt; Math.Sqrt(Math.Pow(p.X - currentPixel.X, 2) + Math.Pow(p.Y - currentPixel.Y, 2))) // Euclidean distance
            .FirstOrDefault();

        if (nearestNeighbor == null)  // No more neighbors found
        {
            break;
        }

        contour.Add(nearestNeighbor);
        contourPixels.Remove(nearestNeighbor);
        currentPixel = nearestNeighbor;
    }

    return contour;
}
</ScriptContext>
    <ScriptErrors>[]</ScriptErrors>
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.3</PackageVersion>
      <Id>instance-segmentation</Id>
      <Version>1.0.2</Version>
      <Required>true</Required>
      <Checksum>C249CD2182AD04DD7226B7713540740C</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>912</Address>
    <Name>Instance Segmentation</Name>
    <Description>Detect objects and their contour mask using a pre-trained YOLO model.
</Description>
    <Group>AI - Machine Learning</Group>
    <Features>
      <ProgramFeature>
        <FieldType>checkbox</FieldType>
        <ForDomains />
        <ForTypes>Sensor:Widget.DisplayModule=homegenie/generic/camerainput</ForTypes>
        <Property>ML.InstanceSegmentation</Property>
        <Description>Enable instance segmentation</Description>
      </ProgramFeature>
      <ProgramFeature>
        <FieldType>text</FieldType>
        <ForDomains />
        <ForTypes>Sensor:Widget.DisplayModule=homegenie/generic/camerainput</ForTypes>
        <Property>ML.InstanceSegmentation.TriggerDetect</Property>
        <Description>Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)</Description>
      </ProgramFeature>
    </Features>
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>//
// SETUP block for the Local LLM automation program.
//
// This code is executed once when the program is enabled. Its primary roles are:
// 1. To define the user-configurable options for this program.
// 2. To specify the front-end widget used for the UI.
// 3. To define the conditions and triggers for executing the Main block.
//
// In this case, Program.Run() executes the Main block immediately.
//

var modelsList = LoadModelList(Path.Combine(Data.GetFolder(), CONFIG_FILE_NAME));
var models = string.Join(",", modelsList.Select(item =&gt; $"{item.Value.Name}={item.Key}"));

Program.Setup(() =&gt; {
  // Options, widget and other program related settings should always be defined in
  // the "Setup" delegate.
  Program
    .AddOption(OPTION_MAX_TOKENS, "1536", "Max Tokens (response length)", "slider:64:4096:32")
    .AddOption(OPTION_MODEL_FILE, "", "Model", $"select:{ models }")
    .UseWidget("custom/examples/local-ai-chat");
});

Program.Run();
</ScriptSetup>
    <ScriptSource>//
// MAIN block for the Local LLM automation program.
//
// This code is executed after the SETUP block. It is responsible for:
// 1. Validating user-configured options.
// 2. Initializing the LLamaSharp model, context, and chat session.
// 3. Defining API endpoints to handle model downloads and user prompts.
// 4. Managing the program's lifecycle and memory cleanup.
//

// Flag to signal that the program needs to be reloaded to apply new settings.
var reconfigureRequired = false;

// Register an event handler that listens for changes to any module parameter.
// If a "ConfigureOptions" parameter changes, we trigger a restart.
When.ModuleParameterChanged((module, parameter) =&gt; {
  if (module.Instance == Program.Module &amp;&amp; parameter.Name.StartsWith("ConfigureOptions.")) {
    reconfigureRequired = true;
  }
  return true;
});

// Load model configuration and paths
var modelList = LoadModelList(Path.Combine(Data.GetFolder(), CONFIG_FILE_NAME));
var selectedModel = Program.Option(OPTION_MODEL_FILE).Value;
string localModelFile = Path.Combine(Data.GetFolder(), selectedModel);


// --- API: LLM DOWNLOAD MANAGEMENT ---
// These endpoints are called by the UI to manage the GGUF file download.

Api.Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadStart", (p) =&gt; {
     // Enqueue the download task via System.Utility/DownloadManager
     var model = modelList[selectedModel];
     Program.Emit(FIELD_DOWNLOAD, "Requested");
     var item = JsonConvert.SerializeObject(new {
         Name = model.Name,
         Url = model.Url,
         Path = Data.GetFolder()
     });
     Api.Call("System.Utility/DownloadManager/Enqueue", item);
     return new ResponseStatus(Status.Ok);

}).Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadCancel", (p) =&gt; {
     // Cancel and delete the current download task
     Program.Emit(FIELD_DOWNLOAD, "Pending");
     var item = JsonConvert.SerializeObject(new { Id = Program.Parameter(FIELD_DOWNLOAD_ID)?.Value });
     var res = Api.Call("System.Utility/DownloadManager/Delete", item);
     Program.Emit(FIELD_DOWNLOAD_ID, "Pending");
     Program.Emit(Program.Option(OPTION_MODEL_FILE).Name, "");
     return new ResponseStatus(Status.Ok);

}).Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadPause", (p) =&gt; {
     Program.Emit(FIELD_DOWNLOAD, "Paused");
     var item = JsonConvert.SerializeObject(new { Id = Program.Parameter(FIELD_DOWNLOAD_ID)?.Value });
     var res = Api.Call("System.Utility/DownloadManager/Pause", item);
     return new ResponseStatus(Status.Ok);

}).Handle($"{Program.Module.Domain}/{Program.Module.Address}/DownloadResume", (p) =&gt; {
     var item = JsonConvert.SerializeObject(new { Id = Program.Parameter(FIELD_DOWNLOAD_ID)?.Value });
     var res = Api.Call("System.Utility/DownloadManager/Resume", item);
     return new ResponseStatus(Status.Ok);
});


// Clear previous ID
Program.Emit("LLM.Id", "");


// --- MODEL FILE CHECK &amp; DOWNLOAD LOOP ---
// If the model file does not exist, enter a loop to monitor the download status.
if (!File.Exists(localModelFile)) {

  while (!File.Exists(localModelFile))
  {
    dynamic response = Api.Call("System.Utility/DownloadManager/GetStatus", localModelFile);
    dynamic fileStatus = JsonConvert.DeserializeObject&lt;dynamic&gt;(response);  

    if ((bool)fileStatus?.Success)
    {
      var task = fileStatus?.Data;
      if (task != null)
      {
        // CASE 1: Download in progress. Update UI fields.
        Program
          .Emit(FIELD_DOWNLOAD, task.Status.ToString())
          .Emit(FIELD_DOWNLOAD_ID, task.Id.ToString())
          .Emit(FIELD_DOWNLOAD_TASK, task.DestinationFilePath.ToString())
          .Emit(FIELD_DOWNLOAD_ERROR, task.ErrorMessage.ToString())
          .Emit(FIELD_DOWNLOAD_PROGRESS, task.ProgressPercentage.ToString("F2", CultureInfo.InvariantCulture));
        
        if (task.ProgressPercentage &gt;= 100) 
        {
          break; // Download complete, exit loop
        }
      }
      else if (Program.Parameter(FIELD_DOWNLOAD_TASK)?.Value != localModelFile)
      {
        // CASE 2: No active task. Set status to 'Pending' so the UI shows the "Select Model" button.
        Program
          .Emit(FIELD_DOWNLOAD, "Pending")
          .Emit(FIELD_DOWNLOAD_ID, String.IsNullOrEmpty(selectedModel) ? "Pending" : "")
          .Emit(FIELD_DOWNLOAD_TASK, String.IsNullOrEmpty(selectedModel) ? localModelFile : localModelFile)
          .Emit(FIELD_DOWNLOAD_PROGRESS, "0");
      }
    }

    if (reconfigureRequired) return;

    Pause(2); // Poll status every 2 seconds
  }

  // File exists. Mark as complete and force a reload to initialize the engine.
  Program
    .Emit(FIELD_STATUS_ERROR, "") 
    .Emit(FIELD_DOWNLOAD, "Complete")
    .Emit(FIELD_DOWNLOAD_PROGRESS, "100");

  return;
}
else if (Program.Parameter(FIELD_DOWNLOAD_TASK)?.Value != localModelFile)
{
  // File already exists on startup. Ensure UI reflects "Complete" state.
  Program
    .Emit(FIELD_DOWNLOAD, "Complete")
    .Emit(FIELD_DOWNLOAD_ID, "")
    .Emit(FIELD_DOWNLOAD_PROGRESS, "100")
    .Emit(FIELD_DOWNLOAD_TASK, localModelFile);
}


// --- LLM ENGINE INITIALIZATION ---

Program
  .Emit("LLM.Id", selectedModel)
  .Emit(FIELD_STATUS_ERROR, "")
  .Emit(FIELD_STATUS_INIT, $"Starting");

var templateType = modelList[selectedModel].Template;
var modelPath = localModelFile;

// Core LLamaSharp objects
LLamaWeights model = null;
LLamaContext context = null;
ChatSession session = null;
CancellationTokenSource generationCts = null;

// Define cleanup action to release unmanaged memory (RAM/VRAM)
var dispose = new Action(() =&gt; {
  session = null;
  if (context != null) {
      context.Dispose();
      context = null;
  }
  if (model != null) {
      model.Dispose();
      model = null;
  }
  // Force Garbage Collection to reclaim memory immediately
  GC.Collect();
  GC.WaitForPendingFinalizers();
});


// Validation
if (String.IsNullOrEmpty(modelPath))
{
  Program
    .Notify($"LLM model path not configured.[program_configure]")
    .Emit(FIELD_STATUS_ERROR, "LLM model path not configured.");
}
else if (!File.Exists(modelPath)) 
{
  Program
    .Notify($"File not found: {modelPath}.[program_configure]")
    .Emit(FIELD_STATUS_ERROR, $"File not found: {modelPath}");
}
else
{
  // Register cleanup handler when program stops/restarts
  When.ProgramStopping(() =&gt; {
      dispose();
      return true; 
  });

  // Apply configuration
  LailamaInferenceParams.MaxTokens = (int)Program.Option(OPTION_MAX_TOKENS).DecimalValue;
  var parameters = new ModelParams(modelPath)
  {
      ContextSize = PARAMS_CONTEXT_SIZE,
      GpuLayerCount = PARAMS_GPU_LAYER_SIZE
  };

 try {
  // Load Weights and Context
  model = LLamaWeights.LoadFromFile(parameters);
  context = model.CreateContext(parameters);

  // Initialize Chat Session
  var executor = new InteractiveExecutor(context);
  var chatHistory = new ChatHistory();

  // Create prototype session to manage state resets
  ChatSession prototypeSession =  ChatSession.InitializeSessionFromHistoryAsync(executor, chatHistory).GetAwaiter().GetResult();
  var resetState = prototypeSession.GetSessionState();

  session = new ChatSession(executor);

  // Inject System Prompt
  var systemMessage = new ChatHistory.Message(AuthorRole.System, GetSystemPrompt(templateType)); 
  session.LoadSession(resetState);
  session.History.Messages.Add(systemMessage);


  // --- WARMUP PHASE ---
  // Perform a dummy inference to initialize KV cache and avoid latency on first user request.
  Program.Emit(FIELD_STATUS_INIT, "WarmingUp");
  try {
      var preWarmupState = session.GetSessionState();
      var warmupParams = new InferenceParams() { MaxTokens = 1 };
      Task.Run(async () =&gt; {
          await foreach (var token in session.ChatAsync(new ChatHistory.Message(AuthorRole.User, " "), warmupParams))
          {
             // consume tokens...
          }
      }).GetAwaiter().GetResult();
      // Restore clean state (remove warmup interaction)
      session.LoadSession(preWarmupState);
  } catch (Exception ex) {
      Program.Log.Warn($"Warmup warning: {ex.Message}");
  }
  Program.Emit(FIELD_STATUS_INIT, "Ready");


  // --- API: INFERENCE CONTROL ---

  Api.Handle($"{Program.Module.Domain}/{Program.Module.Address}/Stop", (p) =&gt; {
      // Endpoint to abort current generation
      if (generationCts != null)
      {
          generationCts.Cancel();
          Program.Emit(FIELD_TOKENS_STREAM, "\n\n[STOPPED]");
      }
      return new ResponseStatus(Status.Ok);

  }).Handle($"{Program.Module.Domain}/{Program.Module.Address}/Process", (userInput) =&gt; {

      var request = Api.Parse(userInput);
      var input = BuildUserPrompt(templateType, request.OriginalRequest);
      
      if (!String.IsNullOrEmpty(input))
      {
          if (generationCts != null) generationCts.Dispose();
          generationCts = new CancellationTokenSource();

          // Inject Current Time as metadata (enclosed in double brackets so the model ignores it)
          var now = DateTime.Now.ToString("g", CultureInfo.InvariantCulture); 
          string inputWithTime = $"[[Current Time: {now}]]\n\n{input}";

          // Run inference in a background thread
          Task.Run(async () =&gt; {
            try 
            {
              var sessionChat = session.ChatAsync(new ChatHistory.Message(AuthorRole.User, inputWithTime), LailamaInferenceParams, 
                        cancellationToken: generationCts.Token);

              // SMART BUFFERING STRATEGY
              // We accumulate tokens in a buffer to detect and remove unwanted patterns
              // that might be split across multiple tokens (e.g. split tags).
              string streamBuffer = "";
              
              // Prepare stop lists
              var technicalTags = LailamaInferenceParams.AntiPrompts.Where(x =&gt; x.StartsWith("&lt;")).ToArray();
              var textStops = LailamaInferenceParams.AntiPrompts.Where(x =&gt; !x.StartsWith("&lt;")).ToArray();
              string[] visualArtifacts = { 
                  "/assistant/avatar", 
                  "/avatar", 
                  "![avatar]", 
                  "![image]" 
              };

              ProcessAsync(sessionChat, (token) =&gt; {
                
                if (string.IsNullOrEmpty(token)) return;

                // 1. Append to buffer
                streamBuffer += token;

                // 2. Filter Visual Artifacts (Remove completely)
                foreach (var art in visualArtifacts)
                {
                    if (streamBuffer.Contains(art))
                        streamBuffer = streamBuffer.Replace(art, "");
                }

                // 3. Filter Technical Tags (e.g., &lt;|end_of_sentence|&gt;)
                foreach (var tag in technicalTags)
                {
                    if (streamBuffer.Contains(tag))
                        streamBuffer = streamBuffer.Replace(tag, "");
                }

                // 4. Handle Textual Stops (e.g., "User:", "System:")
                
                // A. Check if buffer ENDS with a stop word (Generation finished)
                foreach (var stop in textStops)
                {
                    if (streamBuffer.EndsWith(stop))
                    {
                        streamBuffer = ""; // Clear buffer to hide the stop word
                        return; 
                    }
                }

                // B. Check if buffer STARTs to form a stop word (Potential split token)
                bool isPotentialDanger = false;
                foreach (var stop in textStops)
                {
                    if (stop.StartsWith(streamBuffer))
                    {
                        isPotentialDanger = true;
                        break;
                    }
                }

                // C. Safe to Emit?
                // Emit if it's not a potential stop word OR if the buffer is getting too long to be one.
                if (!isPotentialDanger || streamBuffer.Length &gt; 25)
                {
                    Program.Emit(FIELD_TOKENS_STREAM, streamBuffer);
                    streamBuffer = ""; 
                }

              }).GetAwaiter().GetResult();
              
              // 5. Final Flush
              // Emit remaining buffer content if valid
              if (!string.IsNullOrEmpty(streamBuffer))
              {
                  foreach (var tag in technicalTags) streamBuffer = streamBuffer.Replace(tag, "");
                  if (!string.IsNullOrWhiteSpace(streamBuffer))
                  {
                      Program.Emit(FIELD_TOKENS_STREAM, streamBuffer);
                  }
              }

            }
            catch (OperationCanceledException)
            {
                Program.Log.Info("Generation stopped by user.");
            }
            catch (Exception ex)
            {
                Program.Emit(FIELD_STATUS_ERROR, ex.Message);
            }

            generationCts?.Dispose();
            generationCts = null;

          });
      }
      return new ResponseStatus(Status.Ok, "Processing: " + input);
  });

  // Clear errors on success
  Program.Emit(FIELD_STATUS_ERROR, "");

 } catch (Exception e) { 
    Program
      .Notify($"ERROR: {e.Message} [program_configure]")
      .Emit(FIELD_STATUS_ERROR, e.Message);
    Pause(5);
  }
}

// Ensure stream field is initialized
Program.Emit(FIELD_TOKENS_STREAM, "");


// --- KEEP-ALIVE LOOP ---
while (Program.IsRunning)
{
    if (reconfigureRequired)
    {
        dispose();
        return; // Triggers HomeGenie restart of the block
    }
    Pause(1);
}</ScriptSource>
    <ScriptContext>/* 
||==============================================================================================
|| LLM Inference Configuration
||==============================================================================================
*/

const string LAILAMA_SYSTEM_PROMPT = """
You are "Lailama", an expert technical assistant running locally on a HomeGenie server.

RULES:
1. **NO INTERNAL MONOLOGUE**: Start the response IMMEDIATELY.
2. **METADATA FILTER**: User inputs start with `[[Current Time: ...]]`. **IGNORE THIS BLOCK COMPLETELY**. Do not greet the time. Do not mention the time unless explicitly asked "What time is it?".
3. **FOCUS**: Reply ONLY to the text that comes AFTER the `[[...]]` block.
4. **LANGUAGE**: Reply in the same language as the user.
5. **STYLE**: Friendly and professional.
""";

/* Options Constants */
const string OPTION_MODEL_PATH = "ModelPath";
const string OPTION_MAX_TOKENS = "MaxTokens";
const string OPTION_MODEL_FILE = "ModelFile";

const string FIELD_TOKENS_STREAM = "LLM.TokenStream";
const string FIELD_STATUS_INIT = "Status.Init";
const string FIELD_STATUS_ERROR = "Status.Error";
const string FIELD_DOWNLOAD = "Status.Download";
const string FIELD_DOWNLOAD_ID = "Status.Download.Id";
const string FIELD_DOWNLOAD_TASK = "Status.Download.Task";
const string FIELD_DOWNLOAD_ERROR = "Status.Download.Error";
const string FIELD_DOWNLOAD_PROGRESS = "Status.Download.Progress";

const string CONFIG_FILE_NAME = "models.yaml";

const int PARAMS_CONTEXT_SIZE = 4096;
const int PARAMS_GPU_LAYER_SIZE = 0; // 0 = CPU only

public class ModelInfo {
  public string Url = "";
  public string Name = "";
  public string Description = "";
  public string Size = "";
  public string Template = "";
}

public static Dictionary&lt;string, ModelInfo&gt; LoadModelList(string configFilePath = CONFIG_FILE_NAME)
{
    string fullPath = configFilePath;
    try
    {
        string yaml = File.ReadAllText(fullPath);
        var deserializer = new DeserializerBuilder().Build();
        var modelList = deserializer.Deserialize&lt;Dictionary&lt;string, ModelInfo&gt;&gt;(yaml);
        return modelList;
    }
    catch (Exception ex)
    {
        return new Dictionary&lt;string, ModelInfo&gt;();
    }
}


InferenceParams LailamaInferenceParams = new InferenceParams
{
    SamplingPipeline = new DefaultSamplingPipeline
    {
        // Temperature: 0.6 is a balanced value for most models
        Temperature = 0.4f, 
        TopP = 0.95f,
        RepeatPenalty = 1.1f 
    },
    MaxTokens = 1024, 
    
    // ROBUST STOP LIST
    // Comprehensive list to handle various models (ChatML, Llama, Phi, etc.)
    AntiPrompts = new string[]
    {
        // ==============================================================================
        // GROUP 1: OFFICIAL STOP TOKENS (EOS)
        // These are the internal signals used by specific models to say "I am done".
        // ==============================================================================
        "&lt;|im_end|&gt;",           // Standard ChatML (Qwen, DeepSeek Coder, Microsoft Phi)
        "&lt;|endoftext|&gt;",        // Generic standard (GPT-2/3 roots)
        "&lt;|eot_id|&gt;",           // Llama 3 (End of Turn)
        "&lt;|eom_id|&gt;",           // Llama 3 (End of Message - rarer but possible)
        "&lt;|end|&gt;",              // Phi-3
        "&lt;end_of_turn&gt;",        // Gemma 2
        "&lt;|end_of_sentence|&gt;",  // DeepSeek V2/V3 (Often emitted as text in GGUF)
        "&lt;|end_of_response|&gt;",  // Newer "Reasoning" models (DeepSeek R1/Granite)

        // ==============================================================================
        // GROUP 2: START OF NEXT TURN TOKENS
        // If the model forgets to stop, it tries to start the next role. We catch it here.
        // ==============================================================================
        "&lt;|im_start|&gt;",         // ChatML: Model tries to start a new block
        "&lt;|start_header_id|&gt;",  // Llama 3: Model tries to write header for user/system
        "&lt;|user|&gt;",             // Phi-3: Model tries to impersonate user
        "&lt;start_of_turn&gt;",      // Gemma: Model tries to start new turn
        "&lt;|begin_of_sentence|&gt;",// DeepSeek: Often marks the start of a new line
        "&lt;|start_response|&gt;",   // Reasoning models: Marks start of final answer
        "Answer: ",             // DeepSeek: Prevents infinite answer loop 

        // --- Assistant Self-Reference Stoppers (DeepSeek) ---
        "Assistant:",   
        "\nAssistant:", 
        "Response:",    
        "\nResponse:",  
        "### Assistant",
        "### Response",

        // ==============================================================================
        // GROUP 3: TEXTUAL HALLUCINATIONS (The "Chat Mode" leftovers)
        // Models like DeepSeek/Mistral often write "User:" plain text when confused.
        // ==============================================================================
        
        // --- User Stoppers ---
        "\nUser:",              // SAFEST: Newline + User + Colon. (Standard chat format)
        "\nUser ",              // Newline + User + Space (Catches "User " without colon)
        "User:",                // Word + Colon (Catches "User:" at start of line without \n)
        "### Instruction",      // Alpaca/Vicuna style stop
        "### User",             // Common variation

        // --- System Stoppers ---
        "\nSystem:",            // Prevents model from hallucinating System instructions
        "System:",              
        
        // --- DeepSeek Specific ---
        // DeepSeek sometimes outputs a double newline before the User tag.
        "\n\nUser:",
        "\n\nSystem:",
        "\n\nAssistant:"
    }
};


/* 
|| Utility methods
*/

string GetSystemPrompt(string templateType, string prompt = LAILAMA_SYSTEM_PROMPT) {
  switch (templateType) {
    case "llama3":
      // Llama 3 uses a specific system header tag
      return $"&lt;|begin_of_text|&gt;&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;\n\n{prompt}\n&lt;|eot_id|&gt;";
    
    case "phi3":
      // Phi-3 does NOT have an official system tag.
      // Best practice is to place it as plain text at the absolute beginning.
      return $"{prompt}\n\n";

    case "gemma":
      // Gemma technically supports a 'system' turn, but it is often ignored.
      // We simulate a user turn followed by a model acknowledgment to force context adherence.
       return $"&lt;start_of_turn&gt;user\n{prompt}&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\nOK.&lt;end_of_turn&gt;\n";

    case "alpaca":
    case "openchat":
      // These models often accept instructions before the first turn
      return $"{prompt}\n\n";

    case "deepseek":
      // DeepSeek Chat V2/V3
      // Insert system prompt at absolute start, after BOS (Begin Of Sentence).
      return $"&lt;|begin_of_sentence|&gt;{prompt}\n\n";

    case "chatml":
    default:
      // Standard ChatML
      return $"&lt;|im_start|&gt;system\n{prompt}\n&lt;|im_end|&gt;\n"; 
  }
}

string BuildUserPrompt(string templateType, string userInput) {
  switch (templateType) {
    case "llama3":
      // Llama 3: Reinforcing instruction to match user's language
      return $"&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;\n\n{userInput}&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;\n\n";
    
    case "phi3":
      // Phi-3: Similar reinforcement strategy
      return $"&lt;|user|&gt;\n{userInput}&lt;|end|&gt;\n&lt;|assistant|&gt;\n";

    case "alpaca":
      return $"### Instruction:\n{userInput}\n\n### Response:\n";

    case "openchat":
      return $"GPT4 Correct User: {userInput}&lt;|end_of_turn|&gt;GPT4 Correct Assistant:";

    case "gemma":
      // Gemma reinforcement
      return "&lt;start_of_turn&gt;user\n" + 
             userInput + 
             "&lt;end_of_turn&gt;\n&lt;start_of_turn&gt;model\n";
    
    case "deepseek":
      // Note: DeepSeek is very sensitive to newlines (\n\n) between User and Assistant
      return $"User:{userInput}\n\nAssistant: ";

    case "chatml":
    default:
      // Pure ChatML
      return $"&lt;|im_start|&gt;user\n{userInput}&lt;|im_end|&gt;\n&lt;|im_start|&gt;assistant\n"; 
  }
}

async Task ProcessAsync(IAsyncEnumerable&lt;string&gt; asyncEnumerable, Action&lt;string&gt; callback)
{
    await foreach (string item in asyncEnumerable)
    {
        callback(item);
    }
}</ScriptContext>
    <ScriptErrors />
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.3</PackageVersion>
      <Id>lailama</Id>
      <Version>1.0.1</Version>
      <Required>true</Required>
      <Checksum>2A994A58AADB032C5A7FD6419D391164</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>940</Address>
    <Name>Lailama</Name>
    <Description>Harness the power of Generative AI on your hardware. Chat, write code, or build advanced automations with the security of a local and private system.</Description>
    <Group>AI - Machine Learning</Group>
    <Features />
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>Program
  .AddFeature(
    "",
    ForCameraInputType,
    ObjectDetection,
    "Enable objects detection",
    "checkbox"
  ).AddFeature(
    "",
    ForCameraInputType,
    ObjectDetectionTrigger,
    "Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)",
    "text"
  ).AddOption(
    YoloModelPath,
    Data.GetFolder() + "/yolo11n.onnx",
    "Path of YOLO model file (.onnx)",
    "text"
  );

Program.Run();
</ScriptSetup>
    <ScriptSource>var inputModules = Modules.WithFeature( ObjectDetection );
// Restart program if configuration has been changed to apply new settings.
When.ModuleParameterChanged( (module, property) =&gt; {
    if (module.Instance == Program.Module &amp;&amp; property.Name.StartsWith("ConfigureOptions."))
    {
        if (Program.IsRunning) Program.Restart();
        return true;
    }
    return true;
});

var modelPath = Program.Option(YoloModelPath)?.Value;
if (String.IsNullOrEmpty(modelPath))
{
    Program.Notify($"Configure the path of YOLO 'detect' model (.onnx file). {OptionButtons}");
    Pause(5);
    return;
}


// For details about this implemention see
// *ML.net* and *YoloSharp* documentation


try
{
    var errorOccurred = false;
    using var detectPredictor = new YoloPredictor(modelPath);
    while (Program.IsRunning)
    {
        if (inputModules.SelectedModules.Count == 0)
        {
            Pause(1);
            continue;
        }
        inputModules.Command("Camera.GetPicture").Submit((m, data) =&gt; {
            try
            {
                var result = detectPredictor.Detect((byte[])data, new YoloConfiguration { Confidence = 0.35f });
                //Console.WriteLine($"Result: {result}");
                //Console.WriteLine($"Speed:  {result.Speed}");
                var module = Modules.InDomain(m.Domain).WithAddress(m.Address).Get();
                if (result.Count() &gt; 0)
                {
                    var jsonResults = "";
                    // Emit "Sensor.ObjectDetect.Subject.Data" event if anything
                    // mathing the configured "TriggerDetect" list
                    // was detected in the scene.
                    string[] matchList = module
                        .Parameter( ObjectDetectionTrigger )?.Value
                        .Split(',').Select(p =&gt; p.Trim())
                        .Where(x =&gt; !string.IsNullOrEmpty(x))
                        .ToArray();
                    if (matchList.Length &gt; 0)
                    {
                        var filtered = new List&lt;Compunet.YoloSharp.Data.Detection&gt;();
                        foreach (var r in result)
                        {
                            var subject = r.Name.Name;
                            if (matchList.Contains(subject))
                            {
                                filtered.Add(r);
                            }
                        }
                        if (filtered.Count &gt; 0)
                        {
                            if (module.Parameter(ObjectDetect).DecimalValue != filtered.Count) 
                            {
                                module.Emit(ObjectDetect, filtered.Count);
                            }
                            else
                            {
                                // update value and timestamp only, do not emit event
                                module.Parameter(ObjectDetect).SetData(filtered.Count);   
                            }
                            foreach (var r in filtered)
                            {
                                module.Emit(ObjectDetectSubject, r);
                            }
                            jsonResults = JsonConvert.SerializeObject(filtered);
                        }
                        else if (module.Parameter(ObjectDetect).DecimalValue != 0 &amp;&amp; module.Parameter(ObjectDetect).IdleTime &gt; 5) 
                        {
                            module.Emit(ObjectDetect, 0);
                        }
                    }
                    else
                    {
                        jsonResults = JsonConvert.SerializeObject(result);
                    }

                    if (jsonResults != "")
                    {
                        // Emit json data for the video player (overlay data)
                        module.Emit(VideoPlayerWidgetOverlayDetect, jsonResults);
                    }
                }
            }
            catch (Exception e)
            {
                errorOccurred = true;
                Console.WriteLine(e.Message);
            }
        });
        if (errorOccurred)
        {
            errorOccurred = false;
            Pause(5);
        }
    }
}
catch (Exception e)
{
    Program.Notify($"Error: {e.Message} {OptionButtons}");
    Pause(5);
    return;
}
</ScriptSource>
    <ScriptContext>const string
OptionButtons = "[program_configure,program_disable]",
YoloModelPath = "Yolo.ModelPath",
ForCameraInputType = "Sensor:Widget.DisplayModule=homegenie/generic/camerainput",
ObjectDetection = "ML.ObjectDetection",
ObjectDetectionTrigger = $"{ObjectDetection}.TriggerDetect",
ObjectDetect = "Sensor.ObjectDetect",
ObjectDetectSubject = "Sensor.ObjectDetect.Subject",
VideoPlayerWidgetOverlayDetect = "Widget.Data.VideoPlayer.Overlay.Detect";
</ScriptContext>
    <ScriptErrors>[]</ScriptErrors>
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.3</PackageVersion>
      <Id>object-detection</Id>
      <Version>1.0.2</Version>
      <Required>true</Required>
      <Checksum>3D2F4F592448A4868B9A61FB4B111C5B</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>910</Address>
    <Name>Object Detection</Name>
    <Description>Detect objects using a pre-trained YOLO model.
</Description>
    <Group>AI - Machine Learning</Group>
    <Features>
      <ProgramFeature>
        <FieldType>checkbox</FieldType>
        <ForDomains />
        <ForTypes>Sensor:Widget.DisplayModule=homegenie/generic/camerainput</ForTypes>
        <Property>ML.ObjectDetection</Property>
        <Description>Enable objects detection</Description>
      </ProgramFeature>
      <ProgramFeature>
        <FieldType>text</FieldType>
        <ForDomains />
        <ForTypes>Sensor:Widget.DisplayModule=homegenie/generic/camerainput</ForTypes>
        <Property>ML.ObjectDetection.TriggerDetect</Property>
        <Description>Comma-separated list of things (in English) that will trigger the alarm. (e.g.: cat, dog, person, backpack, suitcase)</Description>
      </ProgramFeature>
    </Features>
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
  <ProgramBlock>
    <ScriptSetup>Program
  .AddFeature(
    "",
    ForCameraInputType,
    PoseDetection,
    "Enable pose tracking",
    "checkbox"
  ).AddOption(
    "Yolo.ModelPath",
    Data.GetFolder() + "/yolo11n-pose.onnx",
    "Path of YOLO model file (.onnx)",
    "text"
  );

Program.Run();
</ScriptSetup>
    <ScriptSource>var inputModules = Modules.WithFeature(PoseDetection);

// Restart program if configuration has been changed to apply new settings.
When.ModuleParameterChanged( (module, property) =&gt; {
  if (module.Instance == Program.Module &amp;&amp; property.Name.StartsWith("ConfigureOptions."))
  {
    if (Program.IsRunning) Program.Restart();
    return true;
  }
  return true;
});

var yoloModelPath = Program.Option("Yolo.ModelPath")?.Value;
if (String.IsNullOrEmpty(yoloModelPath))
{
    Program.Notify($"Configure the path of YOLO 'pose' model (.onnx file). {OptionButtons}");
    Pause(5);
    return;
}


// For details about this implemention see
// *ML.net* and *YoloSharp* documentation


try
{
    var errorOccurred = false;
    using var detectPredictor = new YoloPredictor(yoloModelPath);
    while (Program.IsRunning)
    {
        if (inputModules.SelectedModules.Count == 0)
        {
            Pause(1);
            continue;
        }
        inputModules.Command("Camera.GetPicture").Submit((m, data) =&gt; {
            try
            {
                var module = Modules.InDomain(m.Domain).WithAddress(m.Address).Get();
                var result = detectPredictor.Pose((byte[])data);
                //Console.WriteLine($"Result: {result}");
                //Console.WriteLine($"Speed:  {result.Speed}");
                if (result.Count() &gt; 0)
                {
                    var jsonResults = JsonConvert.SerializeObject(result);
                    // Emit json data for the video player (overlay data)
                    module.Emit(VideoPlayerWidgetOverlayPose, jsonResults);

                    if (module.Parameter(ObjectDetect).DecimalValue != result.Count) 
                    {
                        module.Emit(ObjectDetect, result.Count);
                    }
                    else
                    {
                        // update value and timestamp only, do not emit event
                        module.Parameter(ObjectDetect).SetData(result.Count);   
                    }

                    foreach (var r in result)
                    {
                        module.Emit(ObjectDetectSubject, r);
                    }
                }
                else if (module.Parameter(ObjectDetect).DecimalValue != 0 &amp;&amp; module.Parameter(ObjectDetect).IdleTime &gt; 5) 
                {
                    module.Emit(ObjectDetect, 0);
                }
            }
            catch (Exception e)
            {
                errorOccurred = true;
                Console.WriteLine(e.Message);
            }
        });
        if (errorOccurred)
        {
            errorOccurred = false;
            Pause(5);
        }
    }
}
catch (Exception e)
{
    Program.Notify($"Error: {e.Message} {OptionButtons}");
    Pause(5);
    return;
}
</ScriptSource>
    <ScriptContext>const string
OptionButtons = "[program_configure,program_disable]",
YoloModelPath = "Yolo.ModelPath",
ForCameraInputType = "Sensor:Widget.DisplayModule=homegenie/generic/camerainput",
PoseDetection = "ML.PoseEstimation",
ObjectDetect = "Sensor.ObjectDetect",
ObjectDetectSubject = "Sensor.ObjectDetect.Subject",
VideoPlayerWidgetOverlayPose = "Widget.Data.VideoPlayer.Overlay.Pose";
</ScriptContext>
    <ScriptErrors>[]</ScriptErrors>
    <Data />
    <PackageInfo>
      <Repository>homegenie</Repository>
      <PackageId>homegenie-ml-ai</PackageId>
      <PackageVersion>1.0.3</PackageVersion>
      <Id>pose-estimation</Id>
      <Version>1.0.2</Version>
      <Required>true</Required>
      <Checksum>D8CA796A879E90B543897366A4702CBF</Checksum>
    </PackageInfo>
    <Domain>HomeAutomation.HomeGenie.Automation</Domain>
    <Address>911</Address>
    <Name>Pose estimation</Name>
    <Description>Keypoint detection via pose estimation, using custom ONNX models or the default pre-trained YOLO model (human pose-specific).</Description>
    <Group>AI - Machine Learning</Group>
    <Features>
      <ProgramFeature>
        <FieldType>checkbox</FieldType>
        <ForDomains />
        <ForTypes>Sensor:Widget.DisplayModule=homegenie/generic/camerainput</ForTypes>
        <Property>ML.PoseEstimation</Property>
        <Description>Enable pose tracking</Description>
      </ProgramFeature>
    </Features>
    <AutoRestartEnabled>false</AutoRestartEnabled>
    <Cloneable>false</Cloneable>
    <Type>csharp</Type>
    <IsEnabled>true</IsEnabled>
  </ProgramBlock>
</ArrayOfProgramBlock>